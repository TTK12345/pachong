# --- æ ¸å¿ƒç¨³å®šæ€§ä¿®å¤: GeventçŒ´å­è¡¥ä¸ ---
# å¿…é¡»åœ¨æ‰€æœ‰å…¶ä»–å¯¼å…¥ä¹‹å‰æ‰§è¡Œï¼Œä»¥å°†æ ‡å‡†åº“è½¬æ¢ä¸ºéé˜»å¡æ¨¡å¼
from gevent import monkey
monkey.patch_all()

from flask import Flask, render_template, request, jsonify, send_file
from flask_socketio import SocketIO, emit, join_room
import os
import threading
import json
from datetime import datetime, timedelta
import zipfile
import tempfile
import traceback
import platform
import uuid
import re
import gevent # å¯¼å…¥geventç”¨äºå¼‚æ­¥sleep
import random # <--- æ–°å¢å¯¼å…¥
import requests # ç”¨äºè°ƒç”¨Jina AI API
from selenium.common.exceptions import TimeoutException # å¯¼å…¥Timeoutå¼‚å¸¸ç±»
import shutil # ç”¨äºæ–‡ä»¶æ“ä½œ
import mimetypes # ç”¨äºæ–‡ä»¶ç±»å‹æ£€æµ‹
from knowledge_config import KNOWLEDGE_BASE_CONFIG, get_knowledge_base_headers, get_mime_type

# å¯¼å…¥é‡æ„åçš„çˆ¬è™«ç±»
import sys
sys.path.append('./demo')
from gz_crawler_refactored import GzCrawler
from mem_gov_crawler_refactored import MemGovCrawler
from standard_text_crawler_refactored import StandardTextCrawler
from system_file_crawler_refactored import SystemFileCrawler
from normative_file_crawler_refactored import NormativeFileCrawler
from flk_crawler_refactored import FlkCrawler
from custom_page_crawler import CustomPageCrawler

app = Flask(__name__)
app.config['SECRET_KEY'] = 'your-secret-key-for-multi-task'

# æ ¹æ®ç³»ç»Ÿé€‰æ‹©ä¸åŒçš„å¼‚æ­¥æ¨¡å¼
if platform.system() == 'Linux':
    socketio = SocketIO(app, cors_allowed_origins="*", async_mode='gevent')
else:
    socketio = SocketIO(app, cors_allowed_origins="*", async_mode='eventlet')

# --- æ–°å¢: æ—¥å¿—æ–‡ä»¶é…ç½® ---
LOGS_DIR = 'logs'
if not os.path.exists(LOGS_DIR):
    os.makedirs(LOGS_DIR)
    print(f"å·²åˆ›å»ºæ—¥å¿—æ–‡ä»¶å¤¹: {LOGS_DIR}")

# --- æ–°å¢: ä»»åŠ¡æ€»ç»“æ–‡ä»¶é…ç½® ---
SUMMARIES_DIR = 'summaries'
if not os.path.exists(SUMMARIES_DIR):
    os.makedirs(SUMMARIES_DIR)
    print(f"å·²åˆ›å»ºä»»åŠ¡æ€»ç»“æ–‡ä»¶å¤¹: {SUMMARIES_DIR}")

# --- æ ¸å¿ƒæ”¹åŠ¨: ä»»åŠ¡ç®¡ç† ---
# ä½¿ç”¨ä¸€ä¸ªå­—å…¸æ¥ç®¡ç†æ‰€æœ‰ä»»åŠ¡
CRAWLER_TASKS = {}
# å­˜å‚¨ä»»åŠ¡æ€»ç»“æŠ¥å‘Šçš„å­—å…¸
TASK_SUMMARIES = {}

# ç»Ÿä¸€çš„çˆ¬è™«ç±»å‹åç§°æ˜ å°„
CRAWLER_TYPE_NAMES = {
    # ä¸­åäººæ°‘å…±å’Œå›½åº”æ€¥ç®¡ç†éƒ¨
    'mem_gz': 'åº”æ€¥éƒ¨-è§„ç« ',
    'mem_flfg': 'åº”æ€¥éƒ¨-æ³•å¾‹æ³•è§„',
    'mem_gfxwj': 'åº”æ€¥éƒ¨-è§„èŒƒæ€§æ–‡ä»¶', 
    'mem_bzwb': 'åº”æ€¥éƒ¨-æ ‡å‡†æ–‡æœ¬',
    'mem_zdwj': 'åº”æ€¥éƒ¨-åˆ¶åº¦æ–‡ä»¶',
    
    # å›½å®¶æ³•å¾‹æ³•è§„æ•°æ®åº“
    'flk_xf': 'æ³•è§„åº“-å®ªæ³•',
    'flk_fl': 'æ³•è§„åº“-æ³•å¾‹',
    'flk_xzfg': 'æ³•è§„åº“-è¡Œæ”¿æ³•è§„',
    'flk_jcfg': 'æ³•è§„åº“-ç›‘å¯Ÿæ³•è§„',
    'flk_sfjs': 'æ³•è§„åº“-å¸æ³•è§£é‡Š',
    'flk_dfxfg': 'æ³•è§„åº“-åœ°æ–¹æ€§æ³•è§„',
    
    # å…¶ä»–
    'custom': 'è‡ªå®šä¹‰é¡µé¢'
}

# ç»Ÿä¸€çš„ä¸‹è½½ç›®å½•é…ç½®
DOWNLOAD_DIRS = {
    # ä¸­åäººæ°‘å…±å’Œå›½åº”æ€¥ç®¡ç†éƒ¨
    "åº”æ€¥éƒ¨-è§„ç« ": "./åº”æ€¥éƒ¨-è§„ç« ",
    "åº”æ€¥éƒ¨-æ³•å¾‹æ³•è§„": "./åº”æ€¥éƒ¨-æ³•å¾‹æ³•è§„", 
    "åº”æ€¥éƒ¨-è§„èŒƒæ€§æ–‡ä»¶": "./åº”æ€¥éƒ¨-è§„èŒƒæ€§æ–‡ä»¶",
    "åº”æ€¥éƒ¨-æ ‡å‡†æ–‡æœ¬": "./åº”æ€¥éƒ¨-æ ‡å‡†æ–‡æœ¬",
    "åº”æ€¥éƒ¨-åˆ¶åº¦æ–‡ä»¶": "./åº”æ€¥éƒ¨-åˆ¶åº¦æ–‡ä»¶",
    
    # å›½å®¶æ³•å¾‹æ³•è§„æ•°æ®åº“
    "æ³•è§„åº“-å®ªæ³•": "./æ³•è§„åº“-å®ªæ³•",
    "æ³•è§„åº“-æ³•å¾‹": "./æ³•è§„åº“-æ³•å¾‹",
    "æ³•è§„åº“-è¡Œæ”¿æ³•è§„": "./æ³•è§„åº“-è¡Œæ”¿æ³•è§„",
    "æ³•è§„åº“-ç›‘å¯Ÿæ³•è§„": "./æ³•è§„åº“-ç›‘å¯Ÿæ³•è§„", 
    "æ³•è§„åº“-å¸æ³•è§£é‡Š": "./æ³•è§„åº“-å¸æ³•è§£é‡Š",
    "æ³•è§„åº“-åœ°æ–¹æ€§æ³•è§„": "./æ³•è§„åº“-åœ°æ–¹æ€§æ³•è§„",
    
    # å…¶ä»–
    "è‡ªå®šä¹‰é¡µé¢": "./è‡ªå®šä¹‰é¡µé¢"
}

def save_summary_to_file(task_id, summary_data):
    """å°†ä»»åŠ¡æ€»ç»“ä¿å­˜åˆ°æ–‡ä»¶"""
    try:
        summary_file_path = os.path.join(SUMMARIES_DIR, f"{task_id}_summary.json")
        with open(summary_file_path, 'w', encoding='utf-8') as f:
            json.dump(summary_data, f, ensure_ascii=False, indent=2)
        print(f"[DEBUG] ä»»åŠ¡æ€»ç»“å·²ä¿å­˜åˆ°æ–‡ä»¶: {summary_file_path}")
        return True
    except Exception as e:
        print(f"[ERROR] ä¿å­˜ä»»åŠ¡æ€»ç»“åˆ°æ–‡ä»¶å¤±è´¥: {e}")
        return False

# ä¸´æ—¶ä¿®å¤ï¼šé‡å†™load_summaries_from_fileså‡½æ•°
def load_summaries_from_files_fixed():
    """ä»æ–‡ä»¶åŠ è½½æ‰€æœ‰ä»»åŠ¡æ€»ç»“ - ä¿®å¤ç‰ˆæœ¬"""
    global TASK_SUMMARIES
    TASK_SUMMARIES = {}
    try:
        if not os.path.exists(SUMMARIES_DIR):
            return
        
        for filename in os.listdir(SUMMARIES_DIR):
            if filename.endswith('_summary.json'):
                task_id = filename.replace('_summary.json', '')
                file_path = os.path.join(SUMMARIES_DIR, filename)
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        summary_data = json.load(f)
                        TASK_SUMMARIES[task_id] = summary_data
                        print(f"[DEBUG] å·²åŠ è½½ä»»åŠ¡æ€»ç»“: {task_id}")
                except Exception as e:
                    print(f"[ERROR] åŠ è½½ä»»åŠ¡æ€»ç»“æ–‡ä»¶ {filename} å¤±è´¥: {e}")
        
        print(f"[DEBUG] æ€»å…±åŠ è½½äº† {len(TASK_SUMMARIES)} ä¸ªä»»åŠ¡æ€»ç»“")
    except Exception as e:
        print(f"[ERROR] åŠ è½½ä»»åŠ¡æ€»ç»“æ—¶å‘ç”Ÿé”™è¯¯: {e}")

# ä½¿ç”¨ä¿®å¤ç‰ˆæœ¬
load_summaries_from_files = load_summaries_from_files_fixed

def delete_summary_file(task_id):
    """åˆ é™¤ä»»åŠ¡æ€»ç»“æ–‡ä»¶"""
    try:
        summary_file_path = os.path.join(SUMMARIES_DIR, f"{task_id}_summary.json")
        if os.path.exists(summary_file_path):
            os.remove(summary_file_path)
            print(f"[DEBUG] å·²åˆ é™¤ä»»åŠ¡æ€»ç»“æ–‡ä»¶: {summary_file_path}")
            return True
        return False
    except Exception as e:
        print(f"[ERROR] åˆ é™¤ä»»åŠ¡æ€»ç»“æ–‡ä»¶å¤±è´¥: {e}")
        return False

def crawl_custom_page(task_id, page_url, logger, attachment_crawler=None):
    """ä½¿ç”¨Jina AI APIçˆ¬å–è‡ªå®šä¹‰é¡µé¢å¹¶ä¸‹è½½é™„ä»¶"""
    try:
        logger.log("å¼€å§‹è‡ªå®šä¹‰é¡µé¢çˆ¬å–ä»»åŠ¡...")
        
        # æ›´æ–°è¿›åº¦ - æ€»å…±5ä¸ªæ­¥éª¤ï¼šAPIè·å–ã€ä¿å­˜å†…å®¹ã€æœç´¢é™„ä»¶ã€ä¸‹è½½é™„ä»¶ã€å®Œæˆ
        update_task_progress(task_id, {'current': 1, 'total': 5, 'percentage': 20})
        
        # éªŒè¯å’Œè§„èŒƒåŒ–URL
        if not page_url.startswith(('http://', 'https://')):
            page_url = 'https://' + page_url
            logger.log(f"å·²è‡ªåŠ¨æ·»åŠ https://å‰ç¼€: {page_url}")
        
        # ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨Jina AI APIè·å–é¡µé¢å†…å®¹
        logger.log("æ­£åœ¨è¿æ¥Jina AI API...")
        jina_api_url = f"https://r.jina.ai/{page_url}"
        headers = {
            "Authorization": "Bearer jina_cdb5f0355dee4b2ba732fa2c36a8d309tlFOpHzeNtXqc_lja-s9WS4wzIx1",
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Accept": "text/plain, */*"
        }
        
        logger.log(f"æ­£åœ¨çˆ¬å–é¡µé¢: {page_url}")
        logger.log(f"Jina API URL: {jina_api_url}")
        
        # è°ƒç”¨Jina AI API
        response = requests.get(jina_api_url, headers=headers, timeout=60)
        
        # è¯¦ç»†çš„å“åº”è°ƒè¯•ä¿¡æ¯
        logger.log(f"APIå“åº”çŠ¶æ€ç : {response.status_code}")
        if response.status_code != 200:
            logger.log(f"å“åº”å¤´: {dict(response.headers)}")
            logger.log(f"å“åº”å†…å®¹: {response.text[:500]}...")  # åªæ˜¾ç¤ºå‰500å­—ç¬¦
        
        if response.status_code != 200:
            error_msg = f"APIè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}"
            logger.log(error_msg, 'error')
            return {'success': False, 'error': error_msg}
        
        # ç¬¬äºŒæ­¥ï¼šä¿å­˜é¡µé¢å†…å®¹
        update_task_progress(task_id, {'current': 2, 'total': 5, 'percentage': 40})
        
        content = response.text
        logger.log(f"æˆåŠŸè·å–é¡µé¢å†…å®¹ï¼Œé•¿åº¦: {len(content)} å­—ç¬¦")
        
        # æ£€æŸ¥æ˜¯å¦è¢«åœæ­¢ï¼ˆåœ¨ä¿å­˜å‰æ£€æŸ¥ï¼‰
        if attachment_crawler and attachment_crawler.is_stopped:
            logger.log("ä»»åŠ¡å·²è¢«åœæ­¢", 'warning')
            return {'success': False, 'error': 'ä»»åŠ¡å·²è¢«åœæ­¢'}
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        save_dir = "./è‡ªå®šä¹‰é¡µé¢"
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)
            logger.log(f"å·²åˆ›å»ºä¿å­˜ç›®å½•: {save_dir}")
        
        # ç”Ÿæˆé¡µé¢å†…å®¹æ–‡ä»¶åï¼ˆæ›´æ¸…æ™°çš„å‘½åï¼‰
        try:
            from urllib.parse import urlparse
            parsed_url = urlparse(page_url)
            domain = parsed_url.netloc or "unknown_domain"
            # æ¸…ç†åŸŸåä¸­çš„ç‰¹æ®Šå­—ç¬¦
            clean_domain = re.sub(r'[^\w\.-]', '_', domain)
            filename = f"é¡µé¢å†…å®¹_{clean_domain}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        except:
            filename = f"é¡µé¢å†…å®¹_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        
        # ä¿å­˜å†…å®¹åˆ°æ–‡ä»¶
        file_path = os.path.join(save_dir, filename)
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(f"é¡µé¢URL: {page_url}\n")
            f.write(f"çˆ¬å–æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"ä»»åŠ¡ID: {task_id}\n")
            f.write("="*60 + "\n\n")
            f.write(content)
        
        logger.log(f"é¡µé¢å†…å®¹å·²ä¿å­˜åˆ°: {file_path}")
        
        # ç¬¬ä¸‰æ­¥ï¼šæœç´¢å¹¶ä¸‹è½½é™„ä»¶
        update_task_progress(task_id, {'current': 3, 'total': 5, 'percentage': 60})
        
        # æ£€æŸ¥æ˜¯å¦è¢«åœæ­¢ï¼ˆåœ¨æœç´¢é™„ä»¶å‰æ£€æŸ¥ï¼‰
        if attachment_crawler and attachment_crawler.is_stopped:
            logger.log("ä»»åŠ¡å·²è¢«åœæ­¢", 'warning')
            return {'success': False, 'error': 'ä»»åŠ¡å·²è¢«åœæ­¢'}
        
        logger.log("å¼€å§‹æœç´¢é¡µé¢é™„ä»¶...")
        
        # å¦‚æœæ²¡æœ‰ä¼ å…¥çˆ¬è™«å®ä¾‹ï¼Œåˆ™åˆ›å»ºä¸€ä¸ªï¼ˆå‘åå…¼å®¹ï¼‰
        if attachment_crawler is None:
            attachment_crawler = CustomPageCrawler(
                download_path=save_dir,
                logger=logger,
                task_id=task_id,
                socketio=socketio,
                progress_callback=update_task_progress
            )
        
        # è®¾ç½®åŸºç¡€URLç”¨äºå¤„ç†ç›¸å¯¹é“¾æ¥
        attachment_crawler.set_base_url(page_url)
        
        # åˆå§‹åŒ–çˆ¬è™«çš„ç»Ÿè®¡ä¿¡æ¯
        attachment_stats = {
            'total_pages': 1,
            'total_sub_links': 0,
            'total_documents': 1,  # é¡µé¢å†…å®¹æ–‡ä»¶
            'successful_downloads': 1,  # é¡µé¢å†…å®¹æ–‡ä»¶
            'failed_downloads': 0,
            'pages_processed': [{'page_num': 1, 'sub_links_count': 0}],
            'failed_links': []
        }
        
        try:
            # å¯åŠ¨æµè§ˆå™¨é©±åŠ¨
            attachment_crawler.start_driver()
            
            # æœç´¢é™„ä»¶
            attachments = attachment_crawler.get_sub_links(page_url)
            attachment_stats['total_sub_links'] = len(attachments)
            
            # ç¬¬å››æ­¥ï¼šä¸‹è½½é™„ä»¶
            update_task_progress(task_id, {'current': 4, 'total': 5, 'percentage': 80})
            
            if attachments:
                logger.log(f"å¼€å§‹ä¸‹è½½ {len(attachments)} ä¸ªé™„ä»¶...")
                
                for i, attachment in enumerate(attachments):
                    # æ£€æŸ¥æ˜¯å¦è¢«åœæ­¢
                    if attachment_crawler.is_stopped:
                        logger.log("ä»»åŠ¡å·²è¢«åœæ­¢", 'warning')
                        attachment_stats['status'] = 'stopped'
                        break
                        
                    logger.log(f"ä¸‹è½½é™„ä»¶ {i+1}/{len(attachments)}: {attachment.get('title', 'æœªçŸ¥é™„ä»¶')}")
                    
                    # ä¸‹è½½é™„ä»¶
                    success = attachment_crawler.download_from_sublink(attachment)
                    
                    if success:
                        attachment_stats['successful_downloads'] += 1
                        attachment_stats['total_documents'] += 1
                    else:
                        attachment_stats['failed_downloads'] += 1
                        attachment_stats['failed_links'].append({
                            'title': attachment.get('title', 'æœªçŸ¥é™„ä»¶'),
                            'url': attachment.get('url', ''),
                            'error': 'ä¸‹è½½å¤±è´¥'
                        })
                    
                    # æ›´æ–°è¿›åº¦
                    attachment_progress = 80 + (i + 1) / len(attachments) * 15  # 80-95%
                    update_task_progress(task_id, {'current': 4, 'total': 5, 'percentage': attachment_progress})
                
                logger.log(f"é™„ä»¶ä¸‹è½½å®Œæˆï¼ŒæˆåŠŸ: {attachment_stats['successful_downloads']-1}ï¼Œå¤±è´¥: {attachment_stats['failed_downloads']}")
            else:
                logger.log("æœªå‘ç°å¯ä¸‹è½½çš„é™„ä»¶")
                
        except Exception as e:
            logger.log(f"é™„ä»¶ä¸‹è½½è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}", 'warning')
            import traceback
            logger.log(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}", 'warning')
        finally:
            # å…³é—­æµè§ˆå™¨é©±åŠ¨
            if attachment_crawler.driver:
                attachment_crawler.close_driver()
        
        # ç¬¬äº”æ­¥ï¼šå®Œæˆä»»åŠ¡
        update_task_progress(task_id, {'current': 5, 'total': 5, 'percentage': 100})
        
        # ä¿å­˜ä»»åŠ¡æ€»ç»“
        save_custom_page_summary(task_id, page_url, file_path, content, attachment_stats)
        
        logger.log("è‡ªå®šä¹‰é¡µé¢çˆ¬å–ä»»åŠ¡å®Œæˆï¼", 'success')
        return {
            'success': True, 
            'file_path': file_path, 
            'content_length': len(content),
            'attachments_found': attachment_stats['total_sub_links'],
            'attachments_downloaded': attachment_stats['successful_downloads'] - 1  # å‡å»é¡µé¢å†…å®¹æ–‡ä»¶
        }
            
    except requests.exceptions.Timeout:
        error_msg = "è¯·æ±‚è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åé‡è¯•"
        logger.log(error_msg, 'error')
        return {'success': False, 'error': error_msg}
    except requests.exceptions.RequestException as e:
        error_msg = f"ç½‘ç»œè¯·æ±‚é”™è¯¯: {str(e)}"
        logger.log(error_msg, 'error')
        return {'success': False, 'error': error_msg}
    except Exception as e:
        error_msg = f"çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"
        logger.log(error_msg, 'error')
        import traceback
        logger.log(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}", 'error')
        return {'success': False, 'error': error_msg}

def save_custom_page_summary(task_id, page_url, file_path, content, stats):
    """ä¿å­˜è‡ªå®šä¹‰é¡µé¢çˆ¬å–çš„ä»»åŠ¡æ€»ç»“"""
    try:
        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Šæ–‡æœ¬
        summary_lines = []
        
        def add_line(txt):
            summary_lines.append(txt)
        
        add_line("\n" + "="*60)
        add_line("è‡ªå®šä¹‰é¡µé¢çˆ¬å–æ€»ç»“æŠ¥å‘Š")
        add_line("="*60)
        add_line(f"\nğŸŒ é¡µé¢ä¿¡æ¯:")
        add_line(f"   é¡µé¢URL: {page_url}")
        add_line(f"   çˆ¬å–æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        add_line(f"   ä»»åŠ¡ID: {task_id}")
        
        add_line(f"\nğŸ“Š å†…å®¹ç»Ÿè®¡:")
        add_line(f"   é¡µé¢å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
        add_line(f"   é¡µé¢ä¿å­˜æ–‡ä»¶: {file_path}")
        
        # é™„ä»¶ç»Ÿè®¡ä¿¡æ¯
        attachments_found = stats.get('total_sub_links', 0)
        successful_attachments = stats.get('successful_downloads', 1) - 1  # å‡å»é¡µé¢å†…å®¹æ–‡ä»¶
        failed_attachments = stats.get('failed_downloads', 0)
        
        add_line(f"\nğŸ“ é™„ä»¶ç»Ÿè®¡:")
        add_line(f"   å‘ç°é™„ä»¶æ•°é‡: {attachments_found}")
        add_line(f"   æˆåŠŸä¸‹è½½é™„ä»¶: {successful_attachments}")
        if failed_attachments > 0:
            add_line(f"   ä¸‹è½½å¤±è´¥é™„ä»¶: {failed_attachments}")
        
        add_line(f"\nğŸ“ æ–‡ä»¶ä¿å­˜ä½ç½®: ./è‡ªå®šä¹‰é¡µé¢")
        
        add_line("\nğŸ¯ ä»»åŠ¡æ€»ç»“:")
        add_line("   âœ… æˆåŠŸçˆ¬å–äº† 1 ä¸ªè‡ªå®šä¹‰é¡µé¢")
        add_line("   âœ… é¡µé¢å†…å®¹å·²ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶")
        if attachments_found > 0:
            add_line(f"   ğŸ“ å‘ç°å¹¶å¤„ç†äº† {attachments_found} ä¸ªé™„ä»¶")
            if successful_attachments > 0:
                add_line(f"   âœ… æˆåŠŸä¸‹è½½äº† {successful_attachments} ä¸ªé™„ä»¶")
            if failed_attachments > 0:
                add_line(f"   âŒ {failed_attachments} ä¸ªé™„ä»¶ä¸‹è½½å¤±è´¥")
        else:
            add_line("   ğŸ“ æœªå‘ç°å¯ä¸‹è½½çš„é™„ä»¶")
        
        # æ˜¾ç¤ºå¤±è´¥çš„é™„ä»¶ä¿¡æ¯
        failed_links = stats.get('failed_links', [])
        if failed_links:
            add_line(f"\nâŒ ä¸‹è½½å¤±è´¥çš„é™„ä»¶:")
            for i, failed_link in enumerate(failed_links, 1):
                add_line(f"   {i}. {failed_link.get('title', 'æœªçŸ¥é™„ä»¶')} - {failed_link.get('error', 'æœªçŸ¥é”™è¯¯')}")
        
        add_line("\n" + "="*60)
        add_line("è‡ªå®šä¹‰é¡µé¢çˆ¬å–ä»»åŠ¡å®Œæˆï¼")
        add_line("="*60)
        
        summary_text = "\n".join(summary_lines)
        
        # ä¿å­˜åˆ°TASK_SUMMARIES
        TASK_SUMMARIES[task_id] = {
            'task_id': task_id,
            'summary': summary_text,
            'stats': stats,
            'end_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'crawler_type': 'custom',
            'crawler_name': 'è‡ªå®šä¹‰é¡µé¢',
            'save_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        
        # åŒæ—¶ä¿å­˜åˆ°æ–‡ä»¶
        save_summary_to_file(task_id, TASK_SUMMARIES[task_id])
        
        print(f"[DEBUG] å·²ä¿å­˜è‡ªå®šä¹‰é¡µé¢ä»»åŠ¡ {task_id} çš„æ€»ç»“æŠ¥å‘Š")
            
    except Exception as e:
        print(f"[ERROR] ä¿å­˜è‡ªå®šä¹‰é¡µé¢æ€»ç»“æ—¶å‡ºé”™: {e}")

class WebSocketLogger:
    """ç”¨äºå‘ç‰¹å®šä»»åŠ¡æˆ¿é—´å‘é€æ—¥å¿—å¹¶å†™å…¥æ–‡ä»¶çš„ç±»"""
    def __init__(self, socketio, task_id):
        self.socketio = socketio
        self.task_id = task_id
        self.log_file_path = os.path.join(LOGS_DIR, f"{self.task_id}.log")
        try:
            # ä»¥UTF-8ç¼–ç è¿½åŠ æ¨¡å¼æ‰“å¼€æ–‡ä»¶
            self.log_file = open(self.log_file_path, 'a', encoding='utf-8')
        except Exception as e:
            print(f"ä¸¥é‡é”™è¯¯: æ— æ³•æ‰“å¼€æ—¥å¿—æ–‡ä»¶ {self.log_file_path}: {e}")
            self.log_file = None
    
    def log(self, message, level='info'):
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        log_data = {
            'timestamp': datetime.now().strftime('%H:%M:%S'), # å‰ç«¯åªæ˜¾ç¤ºæ—¶é—´
            'level': level,
            'message': str(message),
            'task_id': self.task_id  # æ·»åŠ task_idï¼Œè®©å‰ç«¯çŸ¥é“æ¶ˆæ¯æ¥è‡ªå“ªä¸ªä»»åŠ¡
        }
        
        # æ ¼å¼åŒ–æ§åˆ¶å°å’Œæ–‡ä»¶æ—¥å¿—æ¶ˆæ¯
        log_message_full = f"[{timestamp}] [{level.upper()}] [Task: {self.task_id}] {message}"
        
        print(log_message_full) # æ‰“å°åˆ°æ§åˆ¶å°
        self.socketio.emit('log_message', log_data, room=self.task_id) # å‘é€åˆ°å‰ç«¯

        # å†™å…¥æ–‡ä»¶
        if self.log_file:
            try:
                self.log_file.write(log_message_full + '\n')
                self.log_file.flush() # ç¡®ä¿ç«‹å³å†™å…¥
            except Exception as e:
                print(f"é”™è¯¯: æ— æ³•å†™å…¥æ—¥å¿—æ–‡ä»¶ {self.task_id}: {e}")
    
    def close(self):
        """å…³é—­æ—¥å¿—æ–‡ä»¶å¥æŸ„"""
        if self.log_file:
            self.log_file.close()
            self.log_file = None

# BaseWebCrawler ç±»å·²ç»è¢«ç§»é™¤ï¼ŒåŠŸèƒ½å·²é›†æˆåˆ° demo/base_crawler.py ä¸­çš„ BaseCrawler ç±»

# æ‰€æœ‰WebXXXCrawlerç±»å·²è¢«åˆ é™¤ï¼Œç›´æ¥ä½¿ç”¨demoä¸­çš„çˆ¬è™«ç±»

def update_task_progress(task_id, progress_data):
    """æ›´æ–°ä»»åŠ¡è¿›åº¦çš„å›è°ƒå‡½æ•°"""
    if task_id in CRAWLER_TASKS:
        CRAWLER_TASKS[task_id]['progress'] = progress_data

def run_crawler_thread(task_id, crawler_type, max_pages, page_url=None):
    """ç‹¬ç«‹çš„çˆ¬è™«çº¿ç¨‹å‡½æ•°"""
    print(f"[DEBUG] è¿›å…¥çˆ¬è™«çº¿ç¨‹å‡½æ•°: task_id={task_id}, crawler_type={crawler_type}")
    
    task = CRAWLER_TASKS.get(task_id)
    if not task:
        print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°ä»»åŠ¡ {task_id}")
        return

    logger = task['logger']
    try:
        print(f"[DEBUG] çˆ¬è™«çº¿ç¨‹å·²å¯åŠ¨ï¼Œå‡†å¤‡åˆ›å»ºçˆ¬è™«å®ä¾‹...")
        logger.log("çˆ¬è™«çº¿ç¨‹å·²å¯åŠ¨...")
        
        # ç›´æ¥ä½¿ç”¨demoä¸­çš„çˆ¬è™«ç±»ï¼Œä¼ é€’task_idã€socketioå’Œprogress_callbackå‚æ•°
        if crawler_type == 'gz' or crawler_type == 'mem_gz':
            print(f"[DEBUG] åˆ›å»º GzCrawler å®ä¾‹...")
            download_path = "./åº”æ€¥éƒ¨-è§„ç« " if crawler_type == 'mem_gz' else "./è§„ç« "
            crawler = GzCrawler(download_path=download_path, logger=logger, task_id=task_id, socketio=socketio, progress_callback=update_task_progress)
        elif crawler_type == 'memgov' or crawler_type == 'mem_flfg':
            print(f"[DEBUG] åˆ›å»º MemGovCrawler å®ä¾‹...")
            download_path = "./åº”æ€¥éƒ¨-æ³•å¾‹æ³•è§„" if crawler_type == 'mem_flfg' else "./æ³•å¾‹æ³•è§„"
            crawler = MemGovCrawler(download_path=download_path, logger=logger, task_id=task_id, socketio=socketio, progress_callback=update_task_progress)
        elif crawler_type == 'normative_file' or crawler_type == 'mem_gfxwj':
            print(f"[DEBUG] åˆ›å»º NormativeFileCrawler å®ä¾‹...")
            download_path = "./åº”æ€¥éƒ¨-è§„èŒƒæ€§æ–‡ä»¶" if crawler_type == 'mem_gfxwj' else "./è§„èŒƒæ€§æ–‡ä»¶"
            crawler = NormativeFileCrawler(download_path=download_path, logger=logger, task_id=task_id, socketio=socketio, progress_callback=update_task_progress)
        elif crawler_type == 'standard_text' or crawler_type == 'mem_bzwb':
            print(f"[DEBUG] åˆ›å»º StandardTextCrawler å®ä¾‹...")
            download_path = "./åº”æ€¥éƒ¨-æ ‡å‡†æ–‡æœ¬" if crawler_type == 'mem_bzwb' else "./æ ‡å‡†/æ ‡å‡†æ–‡æœ¬"
            crawler = StandardTextCrawler(download_path=download_path, logger=logger, task_id=task_id, socketio=socketio, progress_callback=update_task_progress)
        elif crawler_type == 'system_file' or crawler_type == 'mem_zdwj':
            print(f"[DEBUG] åˆ›å»º SystemFileCrawler å®ä¾‹...")
            download_path = "./åº”æ€¥éƒ¨-åˆ¶åº¦æ–‡ä»¶" if crawler_type == 'mem_zdwj' else "./æ ‡å‡†/åˆ¶åº¦æ–‡ä»¶"
            crawler = SystemFileCrawler(download_path=download_path, logger=logger, task_id=task_id, socketio=socketio, progress_callback=update_task_progress)
        elif crawler_type.startswith('flk_'):
            print(f"[DEBUG] åˆ›å»ºæ³•è§„æ•°æ®åº“çˆ¬è™«å®ä¾‹: {crawler_type}")
            type_name = CRAWLER_TYPE_NAMES.get(crawler_type, 'æœªçŸ¥ç±»å‹')
            download_path = f"./{type_name}"
            crawler = FlkCrawler(
                download_path=download_path, 
                flk_type=crawler_type,
                logger=logger, 
                task_id=task_id, 
                socketio=socketio, 
                progress_callback=update_task_progress
            )
        elif crawler_type == 'custom':
            print(f"[DEBUG] è‡ªå®šä¹‰é¡µé¢çˆ¬å–: {page_url}")
            logger.log(f"å¼€å§‹çˆ¬å–è‡ªå®šä¹‰é¡µé¢: {page_url}")
            
            # åˆ›å»ºè‡ªå®šä¹‰é¡µé¢çˆ¬è™«å®ä¾‹ï¼ˆç”¨äºåœæ­¢åŠŸèƒ½ï¼‰
            save_dir = "./è‡ªå®šä¹‰é¡µé¢"
            custom_crawler = CustomPageCrawler(
                download_path=save_dir,
                logger=logger,
                task_id=task_id,
                socketio=socketio,
                progress_callback=update_task_progress
            )
            
            # å°†çˆ¬è™«å®ä¾‹å­˜å‚¨åˆ°ä»»åŠ¡ä¸­ï¼Œä»¥ä¾¿åœæ­¢åŠŸèƒ½ä½¿ç”¨
            task['crawler'] = custom_crawler
            task['status'] = 'running'
            
            # å‘æ‰€æœ‰å®¢æˆ·ç«¯å‘é€ä»»åŠ¡çŠ¶æ€æ›´æ–°
            socketio.emit('task_status_change', {
                'task_id': task_id,
                'status': 'running',
                'crawler_type': crawler_type
            })
            
            # è°ƒç”¨è‡ªå®šä¹‰é¡µé¢å¤„ç†å‡½æ•°
            custom_crawl_result = crawl_custom_page(task_id, page_url, logger, custom_crawler)
            task['status'] = 'completed'
            task['end_time'] = datetime.now()
            
            # å‘æ‰€æœ‰å®¢æˆ·ç«¯å‘é€ä»»åŠ¡å®Œæˆé€šçŸ¥
            socketio.emit('task_status_change', {
                'task_id': task_id,
                'status': 'completed',
                'crawler_type': crawler_type
            })
            return
        else:
            print(f"[DEBUG] æœªçŸ¥çš„çˆ¬è™«ç±»å‹: {crawler_type}")
            logger.log(f"æœªçŸ¥çš„çˆ¬è™«ç±»å‹: {crawler_type}", 'error')
            task['status'] = 'error'
            return

        print(f"[DEBUG] çˆ¬è™«å®ä¾‹åˆ›å»ºæˆåŠŸï¼Œå¼€å§‹è®¾ç½®ä»»åŠ¡çŠ¶æ€...")
        task['crawler'] = crawler
        task['status'] = 'running'
        
        # å‘æ‰€æœ‰å®¢æˆ·ç«¯å‘é€ä»»åŠ¡çŠ¶æ€æ›´æ–°
        socketio.emit('task_status_change', {
            'task_id': task_id,
            'status': 'running',
            'crawler_type': crawler_type
        })
        
        print(f"[DEBUG] ä»»åŠ¡çŠ¶æ€å·²æ›´æ–°ä¸ºè¿è¡Œä¸­ï¼Œå¼€å§‹æ‰§è¡Œçˆ¬è™«...")
        
        # æ‰§è¡Œçˆ¬è™«ä»»åŠ¡ï¼Œç»Ÿä¸€ä½¿ç”¨å¤šé¡µé¢é€»è¾‘
        if crawler_type == 'gz' or crawler_type == 'mem_gz':
            print(f"[DEBUG] å¼€å§‹æ‰§è¡Œè§„ç« çˆ¬è™«...")
            base_url = "https://www.mem.gov.cn/gk/zfxxgkpt/fdzdgknr/gz11/"
            crawler.crawl_all_pages(base_url, max_pages=max_pages)
        elif crawler_type == 'memgov' or crawler_type == 'mem_flfg':
            print(f"[DEBUG] å¼€å§‹æ‰§è¡Œæ³•å¾‹æ³•è§„çˆ¬è™«...")
            base_url = "https://www.mem.gov.cn/fw/flfgbz/fg/"
            crawler.crawl_all_pages(base_url, max_pages=1)  # å•é¡µé¢ï¼Œmax_pages=1
        elif crawler_type == 'normative_file' or crawler_type == 'mem_gfxwj':
            print(f"[DEBUG] å¼€å§‹æ‰§è¡Œè§„èŒƒæ€§æ–‡ä»¶çˆ¬è™«...")
            base_url = "https://www.mem.gov.cn/fw/flfgbz/gfxwj/"
            crawler.crawl_all_pages(base_url, max_pages=1)  # å•é¡µé¢ï¼Œmax_pages=1
        elif crawler_type == 'standard_text' or crawler_type == 'mem_bzwb':
            print(f"[DEBUG] å¼€å§‹æ‰§è¡Œæ ‡å‡†æ–‡æœ¬çˆ¬è™«...")
            base_url = "https://www.mem.gov.cn/fw/flfgbz/bz/bzwb/"
            crawler.crawl_all_pages(base_url, max_pages=max_pages)
        elif crawler_type == 'system_file' or crawler_type == 'mem_zdwj':
            print(f"[DEBUG] å¼€å§‹æ‰§è¡Œåˆ¶åº¦æ–‡ä»¶çˆ¬è™«...")
            base_url = "https://www.mem.gov.cn/fw/flfgbz/bz/bzgg/"
            crawler.crawl_all_pages(base_url, max_pages=max_pages)
        elif crawler_type.startswith('flk_'):
            print(f"[DEBUG] å¼€å§‹æ‰§è¡Œæ³•è§„æ•°æ®åº“çˆ¬è™«: {crawler_type}")
            # æ³•è§„æ•°æ®åº“çˆ¬è™«é€šè¿‡APIè·å–æ•°æ®ï¼Œä¸éœ€è¦base_url
            base_url = "https://flk.npc.gov.cn/"
            crawler.crawl_all_pages(base_url, max_pages=max_pages)
        
        print(f"[DEBUG] çˆ¬è™«æ‰§è¡Œå®Œæˆï¼Œè®¾ç½®ä»»åŠ¡çŠ¶æ€ä¸ºå®Œæˆ...")
        task['status'] = 'completed'
        task['end_time'] = datetime.now()
        
        # ç›´æ¥ä¿å­˜æ€»ç»“æŠ¥å‘Šåˆ°æ•°æ®ç»“æ„
        if 'crawler' in task and hasattr(task['crawler'], 'stats'):
            try:
                crawler = task['crawler']
                
                # ç”Ÿæˆæ€»ç»“æŠ¥å‘Šæ–‡æœ¬ï¼ˆä¸çˆ¬è™«ç«¯å®Œå…¨ç›¸åŒçš„é€»è¾‘ï¼‰
                summary_lines = []
                
                def add_line(txt):
                    summary_lines.append(txt)
                
                stats = crawler.stats
                add_line("\n" + "="*60)
                add_line("çˆ¬å–ä»»åŠ¡æ€»ç»“æŠ¥å‘Š")
                add_line("="*60)
                add_line("\nğŸ“Š åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯:")
                add_line(f"   æ€»é¡µæ•°: {stats.get('total_pages', 0)}")
                add_line(f"   æ€»å­é“¾æ¥æ•°: {stats.get('total_sub_links', 0)}")
                add_line(f"   æ€»æ–‡æ¡£æ•°: {stats.get('total_documents', 0)}")
                add_line(f"   æˆåŠŸä¸‹è½½æ•°: {stats.get('successful_downloads', 0)}")
                add_line(f"   å¤±è´¥ä¸‹è½½æ•°: {stats.get('failed_downloads', 0)}")
                
                total_docs = stats.get('total_documents', 0)
                successful_docs = stats.get('successful_downloads', 0)
                if total_docs > 0:
                    success_rate = (successful_docs / total_docs) * 100
                    add_line(f"   ä¸‹è½½æˆåŠŸç‡: {success_rate:.1f}%")
                else:
                    add_line(f"   ä¸‹è½½æˆåŠŸç‡: 0.0% (æ— æ–‡æ¡£)")
                
                add_line(f"\nğŸ“ æ–‡ä»¶ä¿å­˜ä½ç½®: {crawler.download_path}")
                
                add_line("\nğŸ“„ é¡µé¢å¤„ç†è¯¦æƒ…:")
                pages_processed = stats.get('pages_processed', [])
                if pages_processed:
                    for page_info in pages_processed:
                        page_num = page_info.get('page_num', 'æœªçŸ¥')
                        sub_links_count = page_info.get('sub_links_count', 0)
                        add_line(f"   ç¬¬{page_num}é¡µ: {sub_links_count}ä¸ªå­é“¾æ¥")
                else:
                    add_line("   æ— é¡µé¢è¢«å¤„ç†")
                
                add_line("\nâŒ å¤±è´¥é“¾æ¥è¯¦æƒ…:")
                failed_links = stats.get('failed_links', [])
                if failed_links:
                    for failed_link in failed_links:
                        title = failed_link.get('title', 'æœªçŸ¥æ ‡é¢˜')
                        reason = failed_link.get('reason', 'æœªçŸ¥åŸå› ')
                        url = failed_link.get('url', 'æœªçŸ¥URL')
                        add_line(f"   - {title}")
                        add_line(f"     åŸå› : {reason}")
                        add_line(f"     URL: {url}")
                else:
                    add_line("   æ— å¤±è´¥é“¾æ¥")
                
                add_line("\nğŸ¯ ä»»åŠ¡æ€»ç»“:")
                if successful_docs > 0:
                    add_line(f"   âœ… æˆåŠŸä¸‹è½½äº† {successful_docs} ä¸ªæ–‡æ¡£")
                if stats.get('failed_downloads', 0) > 0:
                    add_line(f"   âš ï¸  æœ‰ {stats.get('failed_downloads', 0)} ä¸ªæ–‡æ¡£ä¸‹è½½å¤±è´¥")
                if failed_links:
                    add_line(f"   âŒ æœ‰ {len(failed_links)} ä¸ªé“¾æ¥å¤„ç†å¤±è´¥")
                
                add_line("\n" + "="*60)
                add_line("çˆ¬å–ä»»åŠ¡å®Œæˆï¼")
                add_line("="*60)
                
                # ä¿å­˜åˆ°TASK_SUMMARIES
                summary_text = "\n".join(summary_lines)
                
                # è·å–çˆ¬è™«ç±»å‹çš„ä¸­æ–‡åç§°
                crawler_type_names = CRAWLER_TYPE_NAMES
                
                crawler_type_attr = getattr(crawler, 'crawler_type', crawler_type)
                
                TASK_SUMMARIES[task_id] = {
                    'task_id': task_id,
                    'summary': summary_text,
                    'stats': stats,
                    'end_time': task['end_time'].strftime('%Y-%m-%d %H:%M:%S'),
                    'crawler_type': crawler_type_attr,
                    'crawler_name': crawler_type_names.get(crawler_type_attr, 'æœªçŸ¥'),
                    'save_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }
                
                # åŒæ—¶ä¿å­˜åˆ°æ–‡ä»¶
                save_summary_to_file(task_id, TASK_SUMMARIES[task_id])
                
                print(f"[DEBUG] å·²ç›´æ¥ä¿å­˜ä»»åŠ¡ {task_id} çš„æ€»ç»“æŠ¥å‘Šåˆ°æ•°æ®ç»“æ„")
                print(f"[DEBUG] TASK_SUMMARIES ç°åœ¨åŒ…å«: {list(TASK_SUMMARIES.keys())}")

            except Exception as e:
                print(f"[DEBUG] ç›´æ¥ä¿å­˜æ€»ç»“æŠ¥å‘Šæ—¶å‡ºé”™: {e}")
                import traceback
                print(f"[DEBUG] é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        
        # å‘æ‰€æœ‰å®¢æˆ·ç«¯å‘é€ä»»åŠ¡å®Œæˆé€šçŸ¥
        socketio.emit('task_status_change', {
            'task_id': task_id,
            'status': 'completed',
            'crawler_type': crawler_type
        })

    except Exception as e:
        print(f"[DEBUG] çˆ¬è™«æ‰§è¡Œå‡ºé”™: {str(e)}")
        print(f"[DEBUG] é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        task['status'] = 'error'
        task['end_time'] = datetime.now()
        error_msg = f'çˆ¬è™«æ‰§è¡Œå‡ºé”™: {str(e)}'
        logger.log(error_msg, 'error')
        logger.log(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}", 'error')
        
        # å‘æ‰€æœ‰å®¢æˆ·ç«¯å‘é€ä»»åŠ¡é”™è¯¯é€šçŸ¥
        socketio.emit('task_status_change', {
            'task_id': task_id,
            'status': 'error',
            'crawler_type': crawler_type,
            'error_message': error_msg
        })
    finally:
        print(f"[DEBUG] æ¸…ç†ä»»åŠ¡èµ„æº...")
        if 'logger' in task and hasattr(task['logger'], 'close'):
            task['logger'].close()
            logger.log("æ—¥å¿—æ–‡ä»¶å·²å…³é—­ã€‚")
            
        if 'crawler' in task and hasattr(task['crawler'], 'driver') and task['crawler'].driver:
            try:
                task['crawler'].driver.quit()
                logger.log("æµè§ˆå™¨å·²å…³é—­")
            except: pass
        socketio.emit('crawler_completed', {'message': 'çˆ¬è™«ä»»åŠ¡ç»“æŸ'}, room=task_id)
        logger.log("çˆ¬è™«çº¿ç¨‹å·²ç»“æŸã€‚", "info")
        print(f"[DEBUG] çˆ¬è™«çº¿ç¨‹å·²ç»“æŸ: {task_id}")

def cleanup_old_tasks():
    """åå°çº¿ç¨‹ï¼Œå®šæœŸæ¸…ç†æ—§çš„å·²å®Œæˆä»»åŠ¡ä»¥é‡Šæ”¾å†…å­˜"""
    while True:
        gevent.sleep(600)  # æ¯10åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
        now = datetime.now()
        tasks_to_delete = []
        
        try:
            # åˆ›å»ºä»»åŠ¡å­—å…¸çš„å‰¯æœ¬è¿›è¡Œè¿­ä»£ï¼Œé¿å…åœ¨è¿­ä»£æ—¶ä¿®æ”¹å­—å…¸
            for task_id, task_data in list(CRAWLER_TASKS.items()):
                if task_data.get('end_time'):
                    # å¦‚æœä»»åŠ¡å·²ç»“æŸè¶…è¿‡5åˆ†é’Ÿï¼Œåˆ™æ ‡è®°ä¸ºå¾…åˆ é™¤
                    if now - task_data['end_time'] > timedelta(minutes=5):
                        tasks_to_delete.append(task_id)
            
            if tasks_to_delete:
                print(f"[{datetime.now().strftime('%H:%M:%S')}] æ¸…ç† {len(tasks_to_delete)} ä¸ªæ—§ä»»åŠ¡...")
                for task_id in tasks_to_delete:
                    if task_id in CRAWLER_TASKS:
                        del CRAWLER_TASKS[task_id]
        except Exception as e:
            print(f"[{datetime.now().strftime('%H:%M:%S')}] æ¸…ç†ä»»åŠ¡æ—¶å‘ç”Ÿé”™è¯¯: {e}")

# --- API å’Œ Socket.IO è·¯ç”± ---

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/api/start_crawler', methods=['POST'])
def start_crawler():
    data = request.get_json()
    crawler_type = data.get('crawler_type')
    max_pages = data.get('max_pages', 10)
    page_url = data.get('page_url')  # è·å–è‡ªå®šä¹‰é¡µé¢URL
    
    task_id = "task-" + str(uuid.uuid4())
    logger = WebSocketLogger(socketio, task_id)

    # æ·»åŠ æ›´å¤šä»»åŠ¡ä¿¡æ¯
    CRAWLER_TASKS[task_id] = {
        'status': 'starting', 
        'logger': logger, 
        'crawler': None,
        'start_time': datetime.now(), 
        'end_time': None,
        'crawler_type': crawler_type,  # çˆ¬è™«ç±»å‹
        'max_pages': max_pages,       # æœ€å¤§é¡µæ•°
        'page_url': page_url,         # è‡ªå®šä¹‰é¡µé¢URL
        'progress': {'current': 0, 'total': 0, 'percentage': 0, 'task_id': task_id}  # è¿›åº¦ä¿¡æ¯
    }
    logger.log(f"ä»»åŠ¡ {task_id} å·²åˆ›å»ºï¼Œå‡†å¤‡å¯åŠ¨...")
    
    # å¯åŠ¨geventçº¿ç¨‹
    gevent.spawn(run_crawler_thread, task_id, crawler_type, max_pages, page_url)
    
    return jsonify({'success': True, 'message': 'çˆ¬è™«ä»»åŠ¡å·²åˆ›å»º', 'task_id': task_id})

@app.route('/api/stop_crawler', methods=['POST'])
def stop_crawler():
    task_id = request.get_json().get('task_id')
    task = CRAWLER_TASKS.get(task_id)
    if not task:
        return jsonify({'success': False, 'message': 'ä»»åŠ¡ä¸å­˜åœ¨'})
    if task['status'] == 'running' and task.get('crawler'):
        task['crawler'].stop()
        task['status'] = 'stopping'  # æ›´æ–°çŠ¶æ€
        return jsonify({'success': True, 'message': 'åœæ­¢ä¿¡å·å·²å‘é€'})
    return jsonify({'success': False, 'message': 'ä»»åŠ¡ä¸åœ¨è¿è¡ŒçŠ¶æ€ï¼Œæ— æ³•åœæ­¢'})

# æ–°å¢ï¼šè·å–æ‰€æœ‰ä»»åŠ¡çŠ¶æ€çš„API
@app.route('/api/get_all_tasks', methods=['GET'])
def get_all_tasks():
    """è·å–æ‰€æœ‰ä»»åŠ¡çš„çŠ¶æ€ä¿¡æ¯"""
    tasks_info = []
    for task_id, task_data in CRAWLER_TASKS.items():
        # è®¡ç®—è¿è¡Œæ—¶é—´
        if task_data['start_time']:
            if task_data['end_time']:
                duration = task_data['end_time'] - task_data['start_time']
            else:
                duration = datetime.now() - task_data['start_time']
            duration_str = str(duration).split('.')[0]  # å»æ‰å¾®ç§’
        else:
            duration_str = "0:00:00"
        
        # è·å–çˆ¬è™«ç±»å‹çš„ä¸­æ–‡åç§°
        crawler_type_names = CRAWLER_TYPE_NAMES
        
        task_info = {
            'task_id': task_id,
            'status': task_data['status'],
            'crawler_type': task_data.get('crawler_type', 'unknown'),
            'crawler_name': crawler_type_names.get(task_data.get('crawler_type', ''), 'æœªçŸ¥'),
            'start_time': task_data['start_time'].strftime('%Y-%m-%d %H:%M:%S') if task_data['start_time'] else None,
            'end_time': task_data['end_time'].strftime('%Y-%m-%d %H:%M:%S') if task_data['end_time'] else None,
            'duration': duration_str,
            'max_pages': task_data.get('max_pages'),
            'progress': task_data.get('progress', {'current': 0, 'total': 0, 'percentage': 0})
        }
        tasks_info.append(task_info)
    
    # æŒ‰å¼€å§‹æ—¶é—´æ’åºï¼Œæœ€æ–°çš„åœ¨å‰é¢
    tasks_info.sort(key=lambda x: x['start_time'] if x['start_time'] else '', reverse=True)
    
    return jsonify({'tasks': tasks_info})

# æ–°å¢ï¼šè·å–å•ä¸ªä»»åŠ¡è¯¦æƒ…çš„API
@app.route('/api/get_task_detail/<task_id>', methods=['GET'])
def get_task_detail(task_id):
    """è·å–å•ä¸ªä»»åŠ¡çš„è¯¦ç»†ä¿¡æ¯"""
    task = CRAWLER_TASKS.get(task_id)
    if not task:
        return jsonify({'success': False, 'message': 'ä»»åŠ¡ä¸å­˜åœ¨'}), 404
    
    # è®¡ç®—è¿è¡Œæ—¶é—´
    if task['start_time']:
        if task['end_time']:
            duration = task['end_time'] - task['start_time']
        else:
            duration = datetime.now() - task['start_time']
        duration_str = str(duration).split('.')[0]
    else:
        duration_str = "0:00:00"
    
    # è·å–çˆ¬è™«ç±»å‹çš„ä¸­æ–‡åç§°
    crawler_type_names = CRAWLER_TYPE_NAMES
    
    task_detail = {
        'task_id': task_id,
        'status': task['status'],
        'crawler_type': task.get('crawler_type', 'unknown'),
        'crawler_name': crawler_type_names.get(task.get('crawler_type', ''), 'æœªçŸ¥'),
        'start_time': task['start_time'].strftime('%Y-%m-%d %H:%M:%S') if task['start_time'] else None,
        'end_time': task['end_time'].strftime('%Y-%m-%d %H:%M:%S') if task['end_time'] else None,
        'duration': duration_str,
        'max_pages': task.get('max_pages'),
        'progress': task.get('progress', {'current': 0, 'total': 0, 'percentage': 0})
    }
    
    return jsonify({'success': True, 'task': task_detail})

# æ–°å¢ï¼šåˆ é™¤å·²å®Œæˆä»»åŠ¡çš„API
@app.route('/api/delete_task/<task_id>', methods=['DELETE'])
def delete_task(task_id):
    """åˆ é™¤å·²å®Œæˆçš„ä»»åŠ¡"""
    task = CRAWLER_TASKS.get(task_id)
    if not task:
        return jsonify({'success': False, 'message': 'ä»»åŠ¡ä¸å­˜åœ¨'}), 404
    
    if task['status'] in ['running', 'starting']:
        return jsonify({'success': False, 'message': 'ä¸èƒ½åˆ é™¤æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡'}), 400
    
    # å…³é—­logger
    if 'logger' in task and hasattr(task['logger'], 'close'):
        task['logger'].close()
    
    # åˆ é™¤ä»»åŠ¡
    del CRAWLER_TASKS[task_id]
    
    return jsonify({'success': True, 'message': 'ä»»åŠ¡å·²åˆ é™¤'})

# æ–°å¢ï¼šè·å–ä»»åŠ¡ç»Ÿè®¡ä¿¡æ¯çš„API
@app.route('/api/get_tasks_stats', methods=['GET'])
def get_tasks_stats():
    """è·å–ä»»åŠ¡ç»Ÿè®¡ä¿¡æ¯"""
    stats = {
        'total': len(CRAWLER_TASKS),
        'running': 0,
        'completed': 0,
        'error': 0,
        'stopping': 0,
        'starting': 0
    }
    
    for task_data in CRAWLER_TASKS.values():
        status = task_data['status']
        if status in stats:
            stats[status] += 1
    
    return jsonify({'stats': stats})

# è·å–ä»»åŠ¡æ€»ç»“æŠ¥å‘Šçš„API
@app.route('/api/get_task_summaries', methods=['GET'])
def get_task_summaries():
    """è·å–æ‰€æœ‰ä»»åŠ¡çš„æ€»ç»“æŠ¥å‘Šåˆ—è¡¨"""
    try:
        print(f"[DEBUG] æ”¶åˆ°è·å–ä»»åŠ¡æ€»ç»“è¯·æ±‚")
        print(f"[DEBUG] TASK_SUMMARIES å½“å‰åŒ…å« {len(TASK_SUMMARIES)} ä¸ªæ€»ç»“æŠ¥å‘Š")
        print(f"[DEBUG] TASK_SUMMARIES å†…å®¹: {list(TASK_SUMMARIES.keys())}")
        
        summaries = []
        for task_id, summary_data in TASK_SUMMARIES.items():
            summaries.append(summary_data)
        
        # æŒ‰ä¿å­˜æ—¶é—´æ’åºï¼Œæœ€æ–°çš„åœ¨å‰é¢
        summaries.sort(key=lambda x: x.get('save_time', ''), reverse=True)
        
        print(f"[DEBUG] è¿”å› {len(summaries)} ä¸ªæ€»ç»“æŠ¥å‘Š")
        return jsonify({'summaries': summaries})
    
    except Exception as e:
        print(f"è·å–ä»»åŠ¡æ€»ç»“æ—¶å‡ºé”™: {e}")
        return jsonify({'error': str(e), 'summaries': []}), 500

# è·å–å•ä¸ªä»»åŠ¡æ€»ç»“å†…å®¹çš„API
@app.route('/api/get_summary_content/<task_id>', methods=['GET'])
def get_summary_content(task_id):
    """è·å–æŒ‡å®šä»»åŠ¡çš„æ€»ç»“å†…å®¹"""
    try:
        if task_id in TASK_SUMMARIES:
            summary_data = TASK_SUMMARIES[task_id]
            return jsonify({
                'content': summary_data['summary'],
                'task_id': task_id,
                'name': f"ä»»åŠ¡æ€»ç»“ - {summary_data['crawler_name']}"
            })
        else:
            return jsonify({'error': f'ä»»åŠ¡ {task_id} çš„æ€»ç»“ä¸å­˜åœ¨'}), 404
            
    except Exception as e:
        print(f"è·å–æ€»ç»“å†…å®¹æ—¶å‡ºé”™: {e}")
        return jsonify({'error': str(e)}), 500

# åˆ é™¤ä»»åŠ¡æ€»ç»“çš„API
@app.route('/api/delete_summary/<task_id>', methods=['DELETE'])
def delete_summary(task_id):
    """åˆ é™¤æŒ‡å®šçš„ä»»åŠ¡æ€»ç»“"""
    try:
        if task_id in TASK_SUMMARIES:
            # ä»å†…å­˜ä¸­åˆ é™¤
            del TASK_SUMMARIES[task_id]
            # åŒæ—¶åˆ é™¤æ–‡ä»¶
            delete_summary_file(task_id)
            return jsonify({'success': True, 'message': f'ä»»åŠ¡æ€»ç»“ {task_id} å·²åˆ é™¤'})
        else:
            return jsonify({'success': False, 'message': f'ä»»åŠ¡æ€»ç»“ {task_id} ä¸å­˜åœ¨'}), 404
    except Exception as e:
        print(f"åˆ é™¤ä»»åŠ¡æ€»ç»“æ—¶å‡ºé”™: {e}")
        return jsonify({'success': False, 'message': str(e)}), 500

# æ–°å¢ï¼šæ‰¹é‡å¯åŠ¨å¤šä¸ªçˆ¬è™«ä»»åŠ¡çš„API
@app.route('/api/start_multiple_crawlers', methods=['POST'])
def start_multiple_crawlers():
    """æ‰¹é‡å¯åŠ¨å¤šä¸ªçˆ¬è™«ä»»åŠ¡"""
    data = request.get_json()
    crawler_configs = data.get('crawler_configs', [])
    
    if not crawler_configs:
        return jsonify({'success': False, 'message': 'æ²¡æœ‰æä¾›çˆ¬è™«é…ç½®'}), 400
    
    created_tasks = []
    
    for config in crawler_configs:
        crawler_type = config.get('crawler_type')
        max_pages = config.get('max_pages', 10)
        
        if not crawler_type:
            continue
        
        task_id = "task-" + str(uuid.uuid4())
        logger = WebSocketLogger(socketio, task_id)
        
        CRAWLER_TASKS[task_id] = {
            'status': 'starting',
            'logger': logger,
            'crawler': None,
            'start_time': datetime.now(),
            'end_time': None,
            'crawler_type': crawler_type,
            'max_pages': max_pages,
            'progress': {'current': 0, 'total': 0, 'percentage': 0}
        }
        
        logger.log(f"æ‰¹é‡ä»»åŠ¡ {task_id} å·²åˆ›å»ºï¼Œå‡†å¤‡å¯åŠ¨...")
        gevent.spawn(run_crawler_thread, task_id, crawler_type, max_pages)
        
        created_tasks.append({
            'task_id': task_id,
            'crawler_type': crawler_type,
            'max_pages': max_pages
        })
    
    return jsonify({
        'success': True, 
        'message': f'æˆåŠŸåˆ›å»º {len(created_tasks)} ä¸ªçˆ¬è™«ä»»åŠ¡',
        'tasks': created_tasks
    })

# æ–°å¢ï¼šæ‰¹é‡åœæ­¢å¤šä¸ªçˆ¬è™«ä»»åŠ¡çš„API
@app.route('/api/stop_multiple_crawlers', methods=['POST'])
def stop_multiple_crawlers():
    """æ‰¹é‡åœæ­¢å¤šä¸ªçˆ¬è™«ä»»åŠ¡"""
    data = request.get_json()
    task_ids = data.get('task_ids', [])
    
    if not task_ids:
        return jsonify({'success': False, 'message': 'æ²¡æœ‰æä¾›ä»»åŠ¡ID'}), 400
    
    results = []
    
    for task_id in task_ids:
        task = CRAWLER_TASKS.get(task_id)
        if not task:
            results.append({'task_id': task_id, 'success': False, 'message': 'ä»»åŠ¡ä¸å­˜åœ¨'})
            continue
        
        if task['status'] == 'running' and task.get('crawler'):
            task['crawler'].stop()
            task['status'] = 'stopping'
            results.append({'task_id': task_id, 'success': True, 'message': 'åœæ­¢ä¿¡å·å·²å‘é€'})
        else:
            results.append({'task_id': task_id, 'success': False, 'message': 'ä»»åŠ¡ä¸åœ¨è¿è¡ŒçŠ¶æ€'})
    
    successful_stops = sum(1 for r in results if r['success'])
    
    return jsonify({
        'success': True,
        'message': f'æˆåŠŸåœæ­¢ {successful_stops} ä¸ªä»»åŠ¡',
        'results': results
    })

@socketio.on('connect')
def handle_connect():
    print(f"å®¢æˆ·ç«¯å·²è¿æ¥: {request.sid}")
    emit('connected', {'data': 'è¿æ¥æˆåŠŸï¼Œè¯·åŠ å…¥ä»»åŠ¡æˆ¿é—´'})

@socketio.on('disconnect')
def handle_disconnect():
    print(f"å®¢æˆ·ç«¯å·²æ–­å¼€è¿æ¥: {request.sid}")

@socketio.on('join_task_room')
def handle_join_task_room(data):
    task_id = data.get('task_id')
    if task_id in CRAWLER_TASKS:
        join_room(task_id)
        print(f"[DEBUG] å®¢æˆ·ç«¯ {request.sid} å·²åŠ å…¥ä»»åŠ¡æˆ¿é—´: {task_id}")
        print(f"[DEBUG] å½“å‰ä»»åŠ¡çŠ¶æ€: {CRAWLER_TASKS.get(task_id, {}).get('status', 'unknown')}")
        # å‘é€ç¡®è®¤æ¶ˆæ¯åˆ°ä»»åŠ¡æˆ¿é—´
        emit('joined_task_room', {'task_id': task_id, 'message': f'å·²åŠ å…¥ä»»åŠ¡ {task_id} çš„æˆ¿é—´'}, room=task_id)
    else:
        print(f"å®¢æˆ·ç«¯ {request.sid} å°è¯•åŠ å…¥ä¸å­˜åœ¨çš„ä»»åŠ¡æˆ¿é—´: {task_id}")
        emit('log_message', {
            'timestamp': datetime.now().strftime('%H:%M:%S'), 
            'level': 'error',
            'message': f'å°è¯•åŠ å…¥ä¸€ä¸ªä¸å­˜åœ¨çš„ä»»åŠ¡æˆ¿é—´: {task_id}',
            'task_id': task_id
        })

@socketio.on('join_global_room')
def handle_join_global_room():
    """å®¢æˆ·ç«¯åŠ å…¥å…¨å±€æˆ¿é—´ï¼Œç”¨äºæ¥æ”¶æ‰€æœ‰ä»»åŠ¡çš„çŠ¶æ€æ›´æ–°"""
    join_room('global')
    print(f"å®¢æˆ·ç«¯ {request.sid} å·²åŠ å…¥å…¨å±€æˆ¿é—´")
    emit('joined_global_room', {'message': 'å·²åŠ å…¥å…¨å±€æˆ¿é—´ï¼Œå¯ä»¥æ¥æ”¶æ‰€æœ‰ä»»åŠ¡çš„çŠ¶æ€æ›´æ–°'})

@socketio.on('save_task_summary')
def handle_save_task_summary(data):
    """ä¿å­˜ä»»åŠ¡æ€»ç»“æŠ¥å‘Šåˆ°ä¸“é—¨çš„æ•°æ®ç»“æ„"""
    try:
        print(f"[DEBUG] æ”¶åˆ°save_task_summaryäº‹ä»¶")
        print(f"[DEBUG] äº‹ä»¶æ•°æ®keys: {list(data.keys()) if data else 'None'}")
        
        task_id = data.get('task_id')
        summary = data.get('summary')
        stats = data.get('stats', {})
        end_time = data.get('end_time')
        crawler_type = data.get('crawler_type', 'unknown')
        
        print(f"[DEBUG] è§£æçš„æ•°æ®: task_id={task_id}, summaryé•¿åº¦={len(summary) if summary else 0}, crawler_type={crawler_type}")
        
        if task_id and summary:
            # è·å–çˆ¬è™«ç±»å‹çš„ä¸­æ–‡åç§°
            crawler_type_names = CRAWLER_TYPE_NAMES
            
            # ä¿å­˜æ€»ç»“æŠ¥å‘Š
            TASK_SUMMARIES[task_id] = {
                'task_id': task_id,
                'summary': summary,
                'stats': stats,
                'end_time': end_time,
                'crawler_type': crawler_type,
                'crawler_name': crawler_type_names.get(crawler_type, 'æœªçŸ¥'),
                'save_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
            
            # åŒæ—¶ä¿å­˜åˆ°æ–‡ä»¶
            save_summary_to_file(task_id, TASK_SUMMARIES[task_id])
            
            print(f"[DEBUG] å·²ä¿å­˜ä»»åŠ¡ {task_id} çš„æ€»ç»“æŠ¥å‘Šåˆ°æ•°æ®ç»“æ„")
            print(f"[DEBUG] TASK_SUMMARIES ç°åœ¨åŒ…å«: {list(TASK_SUMMARIES.keys())}")
        else:
            print(f"[DEBUG] æ•°æ®ä¸å®Œæ•´ï¼Œæ— æ³•ä¿å­˜ã€‚task_id={task_id}, summaryå­˜åœ¨={bool(summary)}")
            
    except Exception as e:
        print(f"[DEBUG] ä¿å­˜æ€»ç»“æŠ¥å‘Šæ—¶å‡ºé”™: {e}")
        import traceback
        print(f"[DEBUG] é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")

@socketio.on('get_all_tasks_realtime')
def handle_get_all_tasks_realtime():
    """å®æ—¶è·å–æ‰€æœ‰ä»»åŠ¡çŠ¶æ€"""
    tasks_info = []
    for task_id, task_data in CRAWLER_TASKS.items():
        # è®¡ç®—è¿è¡Œæ—¶é—´
        if task_data['start_time']:
            if task_data['end_time']:
                duration = task_data['end_time'] - task_data['start_time']
            else:
                duration = datetime.now() - task_data['start_time']
            duration_str = str(duration).split('.')[0]
        else:
            duration_str = "0:00:00"
        
        # è·å–çˆ¬è™«ç±»å‹çš„ä¸­æ–‡åç§°
        crawler_type_names = CRAWLER_TYPE_NAMES
        
        task_info = {
            'task_id': task_id,
            'status': task_data['status'],
            'crawler_type': task_data.get('crawler_type', 'unknown'),
            'crawler_name': crawler_type_names.get(task_data.get('crawler_type', ''), 'æœªçŸ¥'),
            'start_time': task_data['start_time'].strftime('%Y-%m-%d %H:%M:%S') if task_data['start_time'] else None,
            'end_time': task_data['end_time'].strftime('%Y-%m-%d %H:%M:%S') if task_data['end_time'] else None,
            'duration': duration_str,
            'max_pages': task_data.get('max_pages'),
            'progress': task_data.get('progress', {'current': 0, 'total': 0, 'percentage': 0})
        }
        tasks_info.append(task_info)
    
    # æŒ‰å¼€å§‹æ—¶é—´æ’åº
    tasks_info.sort(key=lambda x: x['start_time'] if x['start_time'] else '', reverse=True)
    
    emit('all_tasks_update', {'tasks': tasks_info})

@app.route('/api/get_files')
def get_files():
    """è·å–æŒ‡å®šç›®å½•ä¸‹çš„æ–‡ä»¶åˆ—è¡¨ã€‚å¦‚æœæœªæŒ‡å®šç›®å½•ï¼Œåˆ™è¿”å›é»˜è®¤ç›®å½•é›†åˆçš„æ‰€æœ‰æ–‡ä»¶ã€‚"""
    dir_param = request.args.get('dir')  # ç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•çš„è·¯å¾„

    files_info = []
    base_dir = os.path.abspath(os.getcwd())

    def collect_files(target_dir, type_label=None):
        for root, _, files in os.walk(target_dir):
            for filename in files:
                file_path = os.path.join(root, filename)
                if not os.path.isfile(file_path):
                    continue
                rel_path = os.path.relpath(file_path, base_dir).replace('\\', '/')
                stat = os.stat(file_path)
                files_info.append({
                    'name': filename,
                    'path': rel_path,
                    'size': stat.st_size,
                    'mtime': datetime.fromtimestamp(stat.st_mtime).strftime('%Y-%m-%d %H:%M:%S'),
                    'type': type_label or os.path.basename(os.path.dirname(file_path))
                })

    if dir_param:
        abs_dir = os.path.abspath(os.path.join(base_dir, dir_param))
        if not abs_dir.startswith(base_dir) or not os.path.exists(abs_dir):
            return jsonify({'error': 'éæ³•ç›®å½•'}), 400
        collect_files(abs_dir)
    else:
        # æ±‡æ€»é¢„å®šä¹‰ç›®å½•
        download_dirs = DOWNLOAD_DIRS
        for type_label, path in download_dirs.items():
            abs_path = os.path.abspath(path)
            if os.path.exists(abs_path):
                collect_files(abs_path, type_label)

    files_info.sort(key=lambda x: x['mtime'], reverse=True)
    return jsonify({'files': files_info})

@app.route('/api/download_file/<path:filepath>')
def download_file(filepath):
    # æ„å»ºå®‰å…¨çš„æ–‡ä»¶è·¯å¾„
    base_dir = os.path.abspath(os.path.dirname(__name__))
    safe_path = os.path.join(base_dir, filepath)
    # æ£€æŸ¥è·¯å¾„æ˜¯å¦ä»ç„¶åœ¨é¡¹ç›®ç›®å½•å†…ï¼Œé˜²æ­¢ç›®å½•éå†æ”»å‡»
    if not safe_path.startswith(base_dir):
        return jsonify({'error': 'éæ³•è·¯å¾„'}), 400
    try:
        return send_file(safe_path, as_attachment=True)
    except Exception as e:
        return jsonify({'error': str(e)}), 404

@app.route('/api/download_all')
def download_all():
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix='.zip') as temp_zip:
            with zipfile.ZipFile(temp_zip.name, 'w', zipfile.ZIP_DEFLATED) as zipf:
                download_dirs = DOWNLOAD_DIRS
                for type, path in download_dirs.items():
                    abs_path = os.path.abspath(path)
                    if not os.path.exists(abs_path): continue
                    for root, _, files in os.walk(abs_path):
                        for file in files:
                            file_path = os.path.join(root, file)
                            # åˆ›å»ºåœ¨zipæ–‡ä»¶ä¸­çš„ç›¸å¯¹è·¯å¾„
                            archive_name = os.path.join(type, os.path.relpath(file_path, abs_path))
                            zipf.write(file_path, archive_name)
            return send_file(temp_zip.name, as_attachment=True, download_name='çˆ¬è™«ä¸‹è½½æ–‡ä»¶.zip')
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/get_logs')
def get_logs():
    log_files_info = []
    if not os.path.exists(LOGS_DIR):
        return jsonify({'logs': []})
    
    try:
        log_files = sorted(
            os.listdir(LOGS_DIR),
            key=lambda f: os.path.getmtime(os.path.join(LOGS_DIR, f)),
            reverse=True
        )
        
        for filename in log_files:
            if filename.endswith('.log'):
                file_path = os.path.join(LOGS_DIR, filename)
                stat = os.stat(file_path)
                log_files_info.append({
                    'name': filename,
                    'size': stat.st_size,
                    'mtime': datetime.fromtimestamp(stat.st_mtime).strftime('%Y-%m-%d %H:%M:%S')
                })
        return jsonify({'logs': log_files_info})
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/get_log_content/<path:filename>')
def get_log_content(filename):
    if '..' in filename or '/' in filename or '\\' in filename:
        return jsonify({'error': 'éæ³•æ–‡ä»¶å'}), 400
    
    safe_path = os.path.join(LOGS_DIR, filename)
    
    try:
        if not os.path.abspath(safe_path).startswith(os.path.abspath(LOGS_DIR)):
            return jsonify({'error': 'éæ³•è·¯å¾„'}), 400
            
        with open(safe_path, 'r', encoding='utf-8') as f:
            content = f.read()
        return jsonify({'name': filename, 'content': content})
    except FileNotFoundError:
        return jsonify({'error': 'æ—¥å¿—æ–‡ä»¶æœªæ‰¾åˆ°'}), 404
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/delete_log/<path:filename>', methods=['DELETE'])
def delete_log(filename):
    if '..' in filename or '/' in filename or '\\' in filename:
        return jsonify({'error': 'éæ³•æ–‡ä»¶å'}), 400
        
    safe_path = os.path.join(LOGS_DIR, filename)
    
    try:
        if not os.path.abspath(safe_path).startswith(os.path.abspath(LOGS_DIR)):
            return jsonify({'error': 'éæ³•è·¯å¾„'}), 400
        
        if os.path.exists(safe_path):
            os.remove(safe_path)
            return jsonify({'success': True, 'message': f'æ—¥å¿—æ–‡ä»¶ {filename} å·²åˆ é™¤'})
        else:
            return jsonify({'success': False, 'message': 'æ–‡ä»¶ä¸å­˜åœ¨'}), 404
            
    except Exception as e:
        return jsonify({'error': str(e)}), 500

# ==================== çŸ¥è¯†åº“ç›¸å…³API ====================

@app.route('/api/get_knowledge_bases', methods=['GET'])
def get_knowledge_bases():
    """è·å–çŸ¥è¯†åº“åˆ—è¡¨"""
    try:
        # æ„å»ºAPI URL
        config = KNOWLEDGE_BASE_CONFIG
        kb_api_url = config["base_url"] + config["list_api"]
        headers = get_knowledge_base_headers()
        
        # è¯·æ±‚å‚æ•°
        params = {
            "page": 1,
            "page_size": 20,
            "keywords": ""
        }
        
        # å‘é€POSTè¯·æ±‚
        response = requests.post(
            kb_api_url, 
            headers=headers, 
            json={}, 
            params=params, 
            timeout=config["timeout"]["list"]
        )
        
        if response.status_code == 200:
            kb_data = response.json()
            if kb_data.get('code') == 0:
                kbs = kb_data.get('data', {}).get('kbs', [])
                return jsonify({
                    'success': True,
                    'kbs': kbs,
                    'total': len(kbs)
                })
            else:
                return jsonify({
                    'success': False,
                    'message': kb_data.get('message', 'è·å–çŸ¥è¯†åº“åˆ—è¡¨å¤±è´¥')
                })
        else:
            return jsonify({
                'success': False,
                'message': f'APIè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}'
            })
            
    except requests.exceptions.Timeout:
        return jsonify({
            'success': False,
            'message': 'è¯·æ±‚çŸ¥è¯†åº“APIè¶…æ—¶'
        })
    except Exception as e:
        print(f"è·å–çŸ¥è¯†åº“åˆ—è¡¨æ—¶å‡ºé”™: {e}")
        return jsonify({
            'success': False,
            'message': f'è·å–çŸ¥è¯†åº“åˆ—è¡¨æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}'
        })

@app.route('/api/upload_to_knowledge_base', methods=['POST'])
def upload_to_knowledge_base():
    """ä¸Šä¼ æ–‡ä»¶åˆ°çŸ¥è¯†åº“"""
    try:
        data = request.get_json()
        kb_id = data.get('kb_id')
        file_path = data.get('file_path')
        file_name = data.get('file_name')
        
        if not kb_id or not file_path or not file_name:
            return jsonify({
                'success': False,
                'message': 'ç¼ºå°‘å¿…è¦å‚æ•°: kb_id, file_path, file_name'
            })
        
        # æ„å»ºå®Œæ•´çš„æ–‡ä»¶è·¯å¾„
        base_dir = os.path.abspath(os.getcwd())
        full_file_path = os.path.join(base_dir, file_path)
        
        # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿æ–‡ä»¶è·¯å¾„åœ¨é¡¹ç›®ç›®å½•å†…
        if not full_file_path.startswith(base_dir):
            return jsonify({
                'success': False,
                'message': 'éæ³•çš„æ–‡ä»¶è·¯å¾„'
            })
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(full_file_path):
            return jsonify({
                'success': False,
                'message': f'æ–‡ä»¶ä¸å­˜åœ¨: {file_path}'
            })
        
        # è·å–é…ç½®
        config = KNOWLEDGE_BASE_CONFIG
        
        # è·å–æ–‡ä»¶å¤§å°
        file_size = os.path.getsize(full_file_path)
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°é™åˆ¶
        max_size = config["file_limits"]["max_size_mb"] * 1024 * 1024
        if file_size > max_size:
            return jsonify({
                'success': False,
                'message': f'æ–‡ä»¶è¿‡å¤§ï¼Œæœ€å¤§æ”¯æŒ{config["file_limits"]["max_size_mb"]}MB'
            })
        
        # æ£€æŸ¥æ–‡ä»¶ç±»å‹æ˜¯å¦æ”¯æŒ
        ext = os.path.splitext(file_name)[1].lower()
        if ext not in config["file_limits"]["allowed_types"]:
            return jsonify({
                'success': False,
                'message': f'ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {ext}ï¼Œæ”¯æŒçš„ç±»å‹: {", ".join(config["file_limits"]["allowed_types"])}'
            })
        
        # ç¡®å®šæ–‡ä»¶çš„MIMEç±»å‹
        mime_type = get_mime_type(file_name)
        
        # å‡†å¤‡ä¸Šä¼ APIè¯·æ±‚
        upload_api_url = config["base_url"] + config["upload_api"]
        headers = get_knowledge_base_headers()
        # ç§»é™¤Content-Typeï¼Œè®©requestsè‡ªåŠ¨å¤„ç†multipart/form-data
        if "Content-Type" in headers:
            del headers["Content-Type"]
        
        # å‡†å¤‡æ–‡ä»¶ä¸Šä¼ çš„multipart/form-data
        with open(full_file_path, 'rb') as file_content:
            files = {
                'file': (file_name, file_content, mime_type)
            }
            data_form = {
                'kb_id': kb_id
            }
            
            # å‘é€ä¸Šä¼ è¯·æ±‚
            response = requests.post(
                upload_api_url,
                headers=headers,
                files=files,
                data=data_form,
                timeout=config["timeout"]["upload"]
            )
        
        if response.status_code == 200:
            upload_result = response.json()
            if upload_result.get('code') == 0:
                uploaded_docs = upload_result.get('data', [])
                return jsonify({
                    'success': True,
                    'message': f'æ–‡ä»¶ "{file_name}" ä¸Šä¼ æˆåŠŸ',
                    'data': uploaded_docs  # è¿”å›åŒ…å«æ–‡æ¡£IDçš„æ•°æ®
                })
            else:
                return jsonify({
                    'success': False,
                    'message': upload_result.get('message', 'ä¸Šä¼ å¤±è´¥')
                })
        else:
            return jsonify({
                'success': False,
                'message': f'ä¸Šä¼ APIè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}'
            })
            
    except requests.exceptions.Timeout:
        return jsonify({
            'success': False,
            'message': 'ä¸Šä¼ è¯·æ±‚è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•'
        })
    except FileNotFoundError:
        return jsonify({
            'success': False,
            'message': f'æ‰¾ä¸åˆ°æ–‡ä»¶: {file_path}'
        })
    except Exception as e:
        print(f"ä¸Šä¼ æ–‡ä»¶åˆ°çŸ¥è¯†åº“æ—¶å‡ºé”™: {e}")
        import traceback
        print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return jsonify({
            'success': False,
            'message': f'ä¸Šä¼ æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}'
        })

@app.route('/api/parse_documents', methods=['POST'])
def parse_documents():
    """è§£æå·²ä¸Šä¼ çš„æ–‡æ¡£"""
    try:
        data = request.get_json()
        doc_ids = data.get('doc_ids', [])
        
        if not doc_ids:
            return jsonify({
                'success': False,
                'message': 'ç¼ºå°‘æ–‡æ¡£IDåˆ—è¡¨'
            })
        
        if not isinstance(doc_ids, list):
            return jsonify({
                'success': False,
                'message': 'æ–‡æ¡£IDå¿…é¡»æ˜¯åˆ—è¡¨æ ¼å¼'
            })
        
        # è·å–é…ç½®
        config = KNOWLEDGE_BASE_CONFIG
        
        # å‡†å¤‡è§£æAPIè¯·æ±‚
        parse_api_url = config["base_url"] + config["parse_api"]
        headers = get_knowledge_base_headers()
        
        # å‡†å¤‡è§£æè¯·æ±‚æ•°æ®
        parse_data = {
            "doc_ids": doc_ids,
            "run": 1,
            "delete": False
        }
        
        # å‘é€è§£æè¯·æ±‚
        response = requests.post(
            parse_api_url,
            headers=headers,
            json=parse_data,
            timeout=config["timeout"]["parse"]  # ä½¿ç”¨è§£æä¸“ç”¨çš„è¶…æ—¶æ—¶é—´
        )
        
        if response.status_code == 200:
            parse_result = response.json()
            if parse_result.get('code') == 0:
                return jsonify({
                    'success': True,
                    'message': f'æˆåŠŸå¯åŠ¨ {len(doc_ids)} ä¸ªæ–‡æ¡£çš„è§£æä»»åŠ¡',
                    'data': parse_result.get('data', True)
                })
            else:
                return jsonify({
                    'success': False,
                    'message': parse_result.get('message', 'è§£æè¯·æ±‚å¤±è´¥')
                })
        else:
            return jsonify({
                'success': False,
                'message': f'è§£æAPIè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}'
            })
            
    except requests.exceptions.Timeout:
        return jsonify({
            'success': False,
            'message': 'è§£æè¯·æ±‚è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•'
        })
    except Exception as e:
        print(f"è§£ææ–‡æ¡£æ—¶å‡ºé”™: {e}")
        import traceback
        print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return jsonify({
            'success': False,
            'message': f'è§£ææ–‡æ¡£æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}'
        })

# ==================== çŸ¥è¯†åº“APIç»“æŸ ====================

# ---------------------------------------------------------------------------
# æ–°å¢: è·å–ä¸‹è½½ç›®å½•æ ‘ç»“æ„ (å¤šçº§æ–‡ä»¶å¤¹å¯¼èˆª)
# ---------------------------------------------------------------------------

def build_dir_tree(path, base_dir):
    """é€’å½’æ„å»ºç›®å½•æ ‘"""
    node = {
        'name': os.path.basename(path),
        'path': os.path.relpath(path, base_dir).replace('\\', '/'),
        'children': []
    }
    try:
        for entry in os.scandir(path):
            if entry.is_dir():
                node['children'].append(build_dir_tree(entry.path, base_dir))
    except PermissionError:
        pass
    return node

@app.route('/api/get_dir_tree')
def get_dir_tree():
    """è¿”å›å¤šçº§ä¸‹è½½ç›®å½•ç»“æ„ï¼Œç”¨äºå‰ç«¯ä¸‹æ‹‰æ¡†å¯¼èˆª"""
    base_dir = os.path.abspath(os.getcwd())
    
    # è·å–æ‰€æœ‰é…ç½®çš„ç›®å½•è·¯å¾„
    roots = []
    for dir_path in DOWNLOAD_DIRS.values():
        abs_path = os.path.abspath(dir_path)
        if abs_path not in roots:
            roots.append(abs_path)

    tree = []
    for root in roots:
        if os.path.exists(root):
            tree.append(build_dir_tree(root, base_dir))

    return jsonify({'tree': tree})

if __name__ == '__main__':
    # å¯åŠ¨åå°æ¸…ç†çº¿ç¨‹
    if platform.system() == 'Linux':
        gevent.spawn(cleanup_old_tasks)
    
    # åŠ è½½å·²ä¿å­˜çš„ä»»åŠ¡æ€»ç»“
    print("æ­£åœ¨åŠ è½½å·²ä¿å­˜çš„ä»»åŠ¡æ€»ç»“...")
    load_summaries_from_files()
    
    # ä» run.py ç§»è¿‡æ¥çš„å¯åŠ¨é€»è¾‘
    print("============================================================")
    print("æ™ºèƒ½æ–‡æ¡£çˆ¬è™«ç³»ç»Ÿ")
    print("============================================================")
    env = 'production' if not app.debug else 'development'
    print(f"ç¯å¢ƒ: {env}")
    print(f"è°ƒè¯•æ¨¡å¼: {app.debug}")
    # åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œå¯ä»¥è€ƒè™‘ç§»é™¤ipæ˜¾ç¤ºæˆ–æ˜¾ç¤º0.0.0.0
    print(f"æœåŠ¡åœ°å€: http://0.0.0.0:5000")
    print("============================================================")
    print("æŒ‰ Ctrl+C åœæ­¢æœåŠ¡")
    print("============================================================")
    
    # ä¸´æ—¶ä¿®å¤ï¼šç¡®ä¿æ²¡æœ‰è¯­æ³•é”™è¯¯
    print("ç³»ç»Ÿå¯åŠ¨å®Œæˆï¼Œæ‰€æœ‰åŠŸèƒ½å·²å°±ç»ª")
    
    socketio.run(app, host='0.0.0.0', port=5000)