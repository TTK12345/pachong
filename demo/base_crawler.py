import time
import os
import re
from abc import ABC, abstractmethod
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.keys import Keys
import requests
from datetime import datetime

class BaseCrawler(ABC):
    """
    åŸºç¡€çˆ¬è™«ç±»ï¼ŒåŒ…å«æ‰€æœ‰çˆ¬è™«çš„å…±åŒåŠŸèƒ½
    """
    
    def __init__(self, download_path, logger=None, task_id=None, socketio=None, progress_callback=None):
        """
        åˆå§‹åŒ–çˆ¬è™«
        :param download_path: ä¸‹è½½æ–‡ä»¶ä¿å­˜è·¯å¾„
        :param logger: æ—¥å¿—è®°å½•å™¨
        :param task_id: ä»»åŠ¡IDï¼ˆWebç‰ˆæœ¬ä½¿ç”¨ï¼‰
        :param socketio: SocketIOå®ä¾‹ï¼ˆWebç‰ˆæœ¬ä½¿ç”¨ï¼‰
        :param progress_callback: è¿›åº¦æ›´æ–°å›è°ƒå‡½æ•°
        """
        self.download_path = os.path.abspath(download_path)
        self.logger = logger
        self.task_id = task_id
        self.socketio = socketio
        self.progress_callback = progress_callback
        self.is_stopped = False  # åœæ­¢æ ‡å¿—
        
        # åˆ›å»ºä¸‹è½½ç›®å½•
        if not os.path.exists(self.download_path):
            os.makedirs(self.download_path)
            self.log(f"å·²åˆ›å»ºä¸‹è½½æ–‡ä»¶å¤¹: {self.download_path}")
        
        # æµè§ˆå™¨ç›¸å…³
        self.driver = None
        self.wait = None
        self.chrome_options = self._setup_chrome_options()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_pages': 0,
            'total_sub_links': 0,
            'total_documents': 0,
            'successful_downloads': 0,
            'failed_downloads': 0,
            'pages_processed': [],
            'failed_links': []
        }
        
        # è¿›åº¦ç»Ÿè®¡
        self.total_sub_links_count = 0  # æ€»å­é“¾æ¥æ•°é‡
        self.completed_sub_links_count = 0  # å·²å®Œæˆå­é“¾æ¥æ•°é‡
    
    def _setup_chrome_options(self):
        """è®¾ç½®Chromeæµè§ˆå™¨é€‰é¡¹"""
        options = Options()
        options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-blink-features=AutomationControlled')
        options.add_argument('--disable-extensions')
        options.add_argument('--disable-gpu')
        options.add_argument('--window-size=1920,1080')
        options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36')
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option('useAutomationExtension', False)
        options.add_experimental_option("prefs", {
            "download.default_directory": self.download_path,
            "download.prompt_for_download": False,
            "download.directory_upgrade": True,
            "safebrowsing.enabled": True,
            "plugins.always_open_pdf_externally": True
        })
        return options

    def log(self, message, level='info'):
        """è®°å½•æ—¥å¿—ä¿¡æ¯"""
        if self.logger:
            self.logger.log(message, level)

    def update_progress(self, current, total, current_file=''):
        """
        æ›´æ–°è¿›åº¦ä¿¡æ¯
        :param current: å½“å‰è¿›åº¦
        :param total: æ€»æ•°
        :param current_file: å½“å‰å¤„ç†çš„æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
        """
        # å¦‚æœä»»åŠ¡è¢«åœæ­¢ï¼Œè¿”å›False
        if self.is_stopped:
            return False
            
        # å¦‚æœæ˜¯Webç‰ˆæœ¬ï¼Œå‘é€WebSocketæ›´æ–°
        if self.task_id and self.socketio:
            progress_data = {
                'current': current,
                'total': total,
                'percentage': round((current / total) * 100, 1) if total > 0 else 0,
                'current_file': current_file,
                'task_id': self.task_id
            }
            
            # ä½¿ç”¨å›è°ƒå‡½æ•°æ›´æ–°CRAWLER_TASKS
            if self.progress_callback:
                self.progress_callback(self.task_id, progress_data)
            
            # å‘é€WebSocketæ›´æ–°
            self.socketio.emit('progress_update', progress_data, room=self.task_id)
            
        return True

    def stop(self):
        """åœæ­¢çˆ¬è™«"""
        self.is_stopped = True
        self.log("æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨åœæ­¢çˆ¬è™«...", 'warning')
    
    def count_all_sub_links(self, base_url, max_pages=None):
        """
        ç»Ÿè®¡æ‰€æœ‰é¡µé¢çš„å­é“¾æ¥æ•°é‡ï¼ˆç»Ÿä¸€å¤šé¡µé¢é€»è¾‘ï¼‰
        :param base_url: åŸºç¡€URL
        :param max_pages: æœ€å¤§é¡µæ•°
        :return: æ€»å­é“¾æ¥æ•°é‡
        """
        try:
            if not self.driver:
                self.start_driver()
            
            total_count = 0
            page_count = 0
            # å½“max_pagesä¸º0æ—¶è¡¨ç¤ºæ— é™åˆ¶ï¼Œè®¾ä¸ºå¾ˆå¤§çš„æ•°
            if max_pages == 0:
                max_pages = 999999
            elif max_pages is None:
                max_pages = 10
            
            while page_count < max_pages:
                current_url = self.generate_page_url(base_url, page_count)
                
                # æ£€æŸ¥é¡µé¢æ˜¯å¦å­˜åœ¨
                if not self.check_page_exists(current_url):
                    if page_count == 0:
                        self.log(f"ç¬¬ä¸€é¡µä¸å­˜åœ¨: {current_url}", 'error')
                    else:
                        self.log(f"ç¬¬ {page_count + 1} é¡µä¸å­˜åœ¨ï¼Œåœæ­¢ç»Ÿè®¡", 'info')
                    break
                
                # è·å–å½“å‰é¡µé¢çš„å­é“¾æ¥
                sub_links = self.get_sub_links(current_url)
                
                if sub_links:
                    page_link_count = len(sub_links)
                    total_count += page_link_count
                    self.log(f"ç¬¬ {page_count + 1} é¡µ: {page_link_count} ä¸ªå­é“¾æ¥", 'info')
                else:
                    self.log(f"ç¬¬ {page_count + 1} é¡µ: 0 ä¸ªå­é“¾æ¥", 'info')
                
                page_count += 1
                
                # å¦‚æœè¿ç»­å‡ é¡µéƒ½æ²¡æœ‰å­é“¾æ¥ï¼Œåˆ™åœæ­¢
                if not sub_links and page_count > 2:
                    self.log("è¿ç»­é¡µé¢æ— å­é“¾æ¥ï¼Œåœæ­¢ç»Ÿè®¡", 'info')
                    break
            
            self.log(f"ç»Ÿè®¡å®Œæˆï¼šå…± {page_count} é¡µï¼Œæ€»è®¡ {total_count} ä¸ªå­é“¾æ¥", 'info')
            return total_count
            
        except Exception as e:
            self.log(f"ç»Ÿè®¡å­é“¾æ¥æ•°é‡æ—¶å‡ºé”™: {e}", 'error')
            return 0
    
    def update_sub_link_progress(self, increment=1):
        """
        æ›´æ–°å­é“¾æ¥è¿›åº¦
        :param increment: å¢åŠ çš„å®Œæˆæ•°é‡
        """
        self.completed_sub_links_count += increment
        self.update_progress(
            current=self.completed_sub_links_count,
            total=self.total_sub_links_count,
            current_file=f"å·²å®Œæˆ {self.completed_sub_links_count}/{self.total_sub_links_count} ä¸ªé“¾æ¥"
        )
    
    def start_driver(self):
        """å¯åŠ¨æµè§ˆå™¨é©±åŠ¨"""
        try:
            self.driver = webdriver.Chrome(options=self.chrome_options)
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            self.wait = WebDriverWait(self.driver, 20)
            self.log("æµè§ˆå™¨é©±åŠ¨å·²å¯åŠ¨")
        except Exception as e:
            self.log(f"å¯åŠ¨æµè§ˆå™¨é©±åŠ¨å¤±è´¥: {e}", 'error')
            raise
    
    def close_driver(self):
        """å…³é—­æµè§ˆå™¨é©±åŠ¨"""
        if self.driver:
            self.driver.quit()
            self.log("æµè§ˆå™¨é©±åŠ¨å·²å…³é—­")
    
    def clean_filename(self, filename):
        """
        æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤ä¸åˆæ³•çš„å­—ç¬¦
        :param filename: åŸå§‹æ–‡ä»¶å
        :return: æ¸…ç†åçš„æ–‡ä»¶å
        """
        # ç§»é™¤æˆ–æ›¿æ¢ä¸åˆæ³•çš„å­—ç¬¦
        filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
        # ç§»é™¤å¤šä½™çš„ç©ºæ ¼
        filename = re.sub(r'\s+', '_', filename.strip())
        # ç§»é™¤å¤šä½™çš„ä¸‹åˆ’çº¿
        filename = re.sub(r'_+', '_', filename)
        filename = filename.strip('_')
        # é™åˆ¶æ–‡ä»¶åé•¿åº¦
        if len(filename) > 100:
            filename = filename[:100]
        return filename
    
    def get_files_in_directory(self):
        """
        è·å–ä¸‹è½½ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶
        :return: æ–‡ä»¶åˆ—è¡¨ï¼ˆæ–‡ä»¶åå’Œä¿®æ”¹æ—¶é—´ï¼‰
        """
        files = []
        try:
            for filename in os.listdir(self.download_path):
                if os.path.isfile(os.path.join(self.download_path, filename)):
                    file_path = os.path.join(self.download_path, filename)
                    files.append({
                        'name': filename,
                        'path': file_path,
                        'mtime': os.path.getmtime(file_path)
                    })
        except Exception as e:
            self.log(f"è·å–æ–‡ä»¶åˆ—è¡¨æ—¶å‡ºé”™: {e}", 'error')
        return files
    
    def wait_for_download_complete(self, initial_files, timeout=30):
        """
        ç­‰å¾…ä¸‹è½½å®Œæˆï¼Œå¹¶è¿”å›æ–°ä¸‹è½½çš„æ–‡ä»¶
        :param initial_files: ä¸‹è½½å‰çš„æ–‡ä»¶åˆ—è¡¨
        :param timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        :return: æ–°ä¸‹è½½çš„æ–‡ä»¶ä¿¡æ¯
        """
        start_time = time.time()
        self.log(f"å¼€å§‹ç­‰å¾…ä¸‹è½½å®Œæˆï¼Œè¶…æ—¶æ—¶é—´: {timeout}ç§’")
        
        while time.time() - start_time < timeout:
            current_files = self.get_files_in_directory()
            
            # æŸ¥æ‰¾æ–°æ–‡ä»¶
            new_files = []
            for current_file in current_files:
                is_new = True
                for initial_file in initial_files:
                    if current_file['name'] == initial_file['name']:
                        is_new = False
                        break
                if is_new:
                    new_files.append(current_file)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ–°æ–‡ä»¶ä¸”æ–‡ä»¶ä¸å†å˜åŒ–ï¼ˆä¸‹è½½å®Œæˆï¼‰
            if new_files:
                self.log(f"æ£€æµ‹åˆ° {len(new_files)} ä¸ªæ–°æ–‡ä»¶")
                # ç­‰å¾…ä¸€æ®µæ—¶é—´ç¡®ä¿æ–‡ä»¶ä¸‹è½½å®Œæˆ
                time.sleep(3)
                stable_files = []
                for new_file in new_files:
                    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸å†å˜åŒ–ï¼ˆä¸‹è½½å®Œæˆï¼‰
                    if not new_file['name'].endswith('.crdownload') and not new_file['name'].endswith('.tmp'):
                        stable_files.append(new_file)
                        self.log(f"å‘ç°ç¨³å®šæ–‡ä»¶: {new_file['name']}")
                
                if stable_files:
                    return stable_files
            
            time.sleep(1)
        
        self.log(f"ç­‰å¾…è¶…æ—¶ï¼Œæœªæ£€æµ‹åˆ°æ–°æ–‡ä»¶", 'warning')
        return []
    
    def rename_downloaded_file(self, file_info, new_name):
        """
        é‡å‘½åä¸‹è½½çš„æ–‡ä»¶
        :param file_info: æ–‡ä»¶ä¿¡æ¯å­—å…¸
        :param new_name: æ–°çš„æ–‡ä»¶åï¼ˆä¸åŒ…å«æ‰©å±•åï¼‰
        """
        try:
            old_path = file_info['path']
            old_name = file_info['name']
            
            # è·å–æ–‡ä»¶æ‰©å±•å
            _, ext = os.path.splitext(old_name)
            if not ext:
                # æ ¹æ®æ–‡ä»¶å†…å®¹æˆ–åç§°åˆ¤æ–­æ‰©å±•å
                if 'pdf' in old_name.lower():
                    ext = '.pdf'
                elif 'doc' in old_name.lower():
                    ext = '.doc'
                elif 'wps' in old_name.lower():
                    ext = '.wps'
                else:
                    ext = '.pdf'  # é»˜è®¤ä¸ºPDFæ ¼å¼
            
            # æ¸…ç†æ–°æ–‡ä»¶å
            clean_name = self.clean_filename(new_name)
            
            # æ„é€ æ–°æ–‡ä»¶è·¯å¾„
            new_filename = f"{clean_name}{ext}"
            new_path = os.path.join(self.download_path, new_filename)
            
            # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œç›´æ¥è¦†ç›–
            if os.path.exists(new_path):
                self.log(f"æ–‡ä»¶å·²å­˜åœ¨ï¼Œå°†è¦†ç›–: {new_filename}")
                os.remove(new_path)
            
            # é‡å‘½åæ–‡ä»¶
            os.rename(old_path, new_path)
            self.log(f"æ–‡ä»¶å·²é‡å‘½å: {old_name} -> {new_filename}")
            return new_path
            
        except Exception as e:
            self.log(f"é‡å‘½åæ–‡ä»¶æ—¶å‡ºé”™: {e}", 'error')
            return file_info['path']
    
    def download_pdf_directly_from_url(self, pdf_url, title, additional_info=""):
        """
        ç›´æ¥ä»URLä¸‹è½½PDFæ–‡ä»¶
        :param pdf_url: PDFæ–‡ä»¶URL
        :param title: æ–‡æ¡£æ ‡é¢˜
        :param additional_info: é¢å¤–ä¿¡æ¯
        :return: æ˜¯å¦ä¸‹è½½æˆåŠŸ
        """
        try:
            self.log(f"å°è¯•ç›´æ¥ä¸‹è½½PDF: {pdf_url}")
            
            # è®¾ç½®è¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
            }
            
            response = requests.get(pdf_url, headers=headers, timeout=30)
            response.raise_for_status()
            
            # ç”Ÿæˆæ–‡ä»¶å
            filename_parts = [title]
            if additional_info:
                filename_parts.append(additional_info)
            
            clean_title = self.clean_filename("_".join(filename_parts))
            filename = f"{clean_title}.pdf"
            filepath = os.path.join(self.download_path, filename)
            
            # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œç›´æ¥è¦†ç›–
            if os.path.exists(filepath):
                self.log(f"æ–‡ä»¶å·²å­˜åœ¨ï¼Œå°†è¦†ç›–: {filename}")
                os.remove(filepath)
            
            # ä¿å­˜æ–‡ä»¶
            with open(filepath, 'wb') as f:
                f.write(response.content)
            
            self.log(f"PDFä¸‹è½½æˆåŠŸ: {filename}", 'success')
            return True
            
        except Exception as e:
            self.log(f"ç›´æ¥ä¸‹è½½PDFå¤±è´¥: {e}", 'error')
            return False
    
    def try_download_buttons(self):
        """
        å°è¯•ç‚¹å‡»å„ç§å¯èƒ½çš„ä¸‹è½½æŒ‰é’®
        """
        download_button_selectors = [
            "//button[contains(text(), 'ä¸‹è½½')]",
            "//a[contains(text(), 'ä¸‹è½½')]",
            "//button[contains(@class, 'download')]",
            "//a[contains(@class, 'download')]",
            "//button//span[contains(text(), 'ä¸‹è½½')]",
            "//div[contains(@class, 'download')]//button",
            "//a[contains(text(), 'PDF')]",
            "//a[contains(text(), 'é™„ä»¶')]",
            "//span[contains(text(), 'ä¸‹è½½')]/parent::a",
            "//div[contains(@class, 'download')]//a"
        ]
        
        for selector in download_button_selectors:
            try:
                download_btn = self.wait.until(
                    EC.element_to_be_clickable((By.XPATH, selector))
                )
                download_btn.click()
                self.log(f"æˆåŠŸç‚¹å‡»ä¸‹è½½æŒ‰é’®: {selector}")
                time.sleep(2)
                return True
            except Exception:
                continue
        
        # å¦‚æœéƒ½å¤±è´¥äº†ï¼Œå°è¯•é”®ç›˜å¿«æ·é”®
        try:
            self.log("å°è¯•ä½¿ç”¨Ctrl+Så¿«æ·é”®...")
            self.driver.find_element(By.TAG_NAME, 'body').send_keys(Keys.CONTROL + 's')
            time.sleep(2)
            return True
        except Exception as e:
            self.log(f"å¿«æ·é”®ä¸‹è½½å¤±è´¥: {e}", 'error')
            return False
    
    def check_page_exists(self, url):
        """
        æ£€æŸ¥é¡µé¢æ˜¯å¦å­˜åœ¨
        :param url: é¡µé¢URL
        :return: é¡µé¢æ˜¯å¦å­˜åœ¨
        """
        try:
            response = requests.get(url, timeout=10)
            return response.status_code == 200
        except:
            return False
    
    def save_page_content(self, url, title):
        """
        ä¿å­˜é¡µé¢å†…å®¹ä¸ºHTMLæ–‡ä»¶
        :param url: é¡µé¢URL
        :param title: é¡µé¢æ ‡é¢˜
        :return: æ˜¯å¦ä¿å­˜æˆåŠŸ
        """
        try:
            self.log("ä¿å­˜é¡µé¢å†…å®¹ä¸ºHTMLæ–‡ä»¶...")
            
            # è·å–é¡µé¢æºç 
            page_source = self.driver.page_source
            
            # ç”Ÿæˆæ–‡ä»¶å
            clean_title = self.clean_filename(title)
            filename = f"{clean_title}.html"
            filepath = os.path.join(self.download_path, filename)
            
            # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œç›´æ¥è¦†ç›–
            if os.path.exists(filepath):
                self.log(f"æ–‡ä»¶å·²å­˜åœ¨ï¼Œå°†è¦†ç›–: {filename}")
            
            # ä¿å­˜æ–‡ä»¶
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(page_source)
            
            self.log(f"é¡µé¢å†…å®¹ä¿å­˜æˆåŠŸ: {filename}", 'success')
            return True
            
        except Exception as e:
            self.log(f"ä¿å­˜é¡µé¢å†…å®¹å¤±è´¥: {e}", 'error')
            return False
    
    def print_summary_report(self):
        """
        è¾“å‡ºè¯¦ç»†çš„æ€»ç»“æŠ¥å‘Š
        """
        self.log("å¼€å§‹ç”Ÿæˆæ€»ç»“æŠ¥å‘Š...", 'info')
        summary_lines = []
        
        def add_line(txt):
            summary_lines.append(txt)
            self.log(txt)
        
        try:
            add_line("\n" + "="*60)
            add_line("çˆ¬å–ä»»åŠ¡æ€»ç»“æŠ¥å‘Š")
            add_line("="*60)
            
            add_line("\nğŸ“Š åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯:")
            add_line(f"   æ€»é¡µæ•°: {self.stats.get('total_pages', 0)}")
            add_line(f"   æ€»å­é“¾æ¥æ•°: {self.stats.get('total_sub_links', 0)}")
            add_line(f"   æ€»æ–‡æ¡£æ•°: {self.stats.get('total_documents', 0)}")
            add_line(f"   æˆåŠŸä¸‹è½½æ•°: {self.stats.get('successful_downloads', 0)}")
            add_line(f"   å¤±è´¥ä¸‹è½½æ•°: {self.stats.get('failed_downloads', 0)}")
            
            total_docs = self.stats.get('total_documents', 0)
            successful_docs = self.stats.get('successful_downloads', 0)
            if total_docs > 0:
                success_rate = (successful_docs / total_docs) * 100
                add_line(f"   ä¸‹è½½æˆåŠŸç‡: {success_rate:.1f}%")
            else:
                add_line(f"   ä¸‹è½½æˆåŠŸç‡: 0.0% (æ— æ–‡æ¡£)")
            
            add_line(f"\nğŸ“ æ–‡ä»¶ä¿å­˜ä½ç½®: {self.download_path}")
            
            add_line("\nğŸ“„ é¡µé¢å¤„ç†è¯¦æƒ…:")
            pages_processed = self.stats.get('pages_processed', [])
            if pages_processed:
                for page_info in pages_processed:
                    page_num = page_info.get('page_num', 'æœªçŸ¥')
                    sub_links_count = page_info.get('sub_links_count', 0)
                    add_line(f"   ç¬¬{page_num}é¡µ: {sub_links_count}ä¸ªå­é“¾æ¥")
            else:
                add_line("   æ— é¡µé¢è¢«å¤„ç†")
            
            add_line("\nâŒ å¤±è´¥é“¾æ¥è¯¦æƒ…:")
            failed_links = self.stats.get('failed_links', [])
            if failed_links:
                for failed_link in failed_links:
                    title = failed_link.get('title', 'æœªçŸ¥æ ‡é¢˜')
                    reason = failed_link.get('reason', 'æœªçŸ¥åŸå› ')
                    url = failed_link.get('url', 'æœªçŸ¥URL')
                    add_line(f"   - {title}")
                    add_line(f"     åŸå› : {reason}")
                    add_line(f"     URL: {url}")
            else:
                add_line("   æ— å¤±è´¥é“¾æ¥")
            
            add_line("\nğŸ¯ ä»»åŠ¡æ€»ç»“:")
            if successful_docs > 0:
                add_line(f"   âœ… æˆåŠŸä¸‹è½½äº† {successful_docs} ä¸ªæ–‡æ¡£")
            if self.stats.get('failed_downloads', 0) > 0:
                add_line(f"   âš ï¸  æœ‰ {self.stats.get('failed_downloads', 0)} ä¸ªæ–‡æ¡£ä¸‹è½½å¤±è´¥")
            if failed_links:
                add_line(f"   âŒ æœ‰ {len(failed_links)} ä¸ªé“¾æ¥å¤„ç†å¤±è´¥")
            
            add_line("\n" + "="*60)
            add_line("çˆ¬å–ä»»åŠ¡å®Œæˆï¼")
            add_line("="*60)
             
            # æ€»ç»“æŠ¥å‘Šç”Ÿæˆå®Œæˆ
            summary_text = "\n".join(summary_lines)
            
            # å°†æ€»ç»“æŠ¥å‘Šä¿å­˜åˆ°ä¸“é—¨çš„æ•°æ®ç»“æ„ä¸­
            if self.logger and hasattr(self.logger, 'socketio'):
                try:
                    self.log(f"å‡†å¤‡å‘é€æ€»ç»“æŠ¥å‘Šäº‹ä»¶ï¼Œtask_id: {self.task_id}", 'info')
                    self.log(f"çˆ¬è™«ç±»å‹: {getattr(self, 'crawler_type', 'unknown')}", 'info')
                    self.log(f"æ€»ç»“æŠ¥å‘Šé•¿åº¦: {len(summary_text)}", 'info')
                    
                    # é€šè¿‡SocketIOäº‹ä»¶å°†æ€»ç»“æŠ¥å‘Šä¿å­˜åˆ°åç«¯æ•°æ®ç»“æ„
                    event_data = {
                        'task_id': self.task_id,
                        'summary': summary_text,
                        'stats': self.stats,
                        'end_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                        'crawler_type': getattr(self, 'crawler_type', 'unknown')
                    }
                    
                    self.logger.socketio.emit('save_task_summary', event_data)
                    self.log("æ€»ç»“æŠ¥å‘Šäº‹ä»¶å·²å‘é€", 'info')
                    
                    # ç­‰å¾…ä¸€ä¸‹ç¡®ä¿äº‹ä»¶è¢«å¤„ç†
                    import time
                    time.sleep(1)
                    self.log("æ€»ç»“æŠ¥å‘Šä¿å­˜å®Œæ¯•", 'info')
                    
                except Exception as e:
                    self.log(f"ä¿å­˜æ€»ç»“æŠ¥å‘Šæ—¶å‡ºé”™: {e}", 'error')
                    import traceback
                    self.log(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}", 'error')
            else:
                self.log("è­¦å‘Š: æ— æ³•å‘é€æ€»ç»“æŠ¥å‘Šï¼Œloggeræˆ–socketioä¸å¯ç”¨", 'warning')
                    
        except Exception as e:
            self.log(f"ç”Ÿæˆæ€»ç»“æŠ¥å‘Šæ—¶å‡ºé”™: {e}", 'error')
            # è¾“å‡ºç®€å•çš„ç»Ÿè®¡ä¿¡æ¯ä½œä¸ºå¤‡ç”¨
            self.log(f"\nç®€å•ç»Ÿè®¡:")
            self.log(f"æ€»é¡µæ•°: {self.stats.get('total_pages', 0)}")
            self.log(f"æ€»å­é“¾æ¥æ•°: {self.stats.get('total_sub_links', 0)}")
            self.log(f"æ€»æ–‡æ¡£æ•°: {self.stats.get('total_documents', 0)}")
            self.log(f"æˆåŠŸä¸‹è½½æ•°: {self.stats.get('successful_downloads', 0)}")
            self.log(f"å¤±è´¥ä¸‹è½½æ•°: {self.stats.get('failed_downloads', 0)}")
    
    # æŠ½è±¡æ–¹æ³•ï¼Œç”±å­ç±»å®ç°
    @abstractmethod
    def get_sub_links(self, main_url):
        """
        è·å–ä¸»é¡µé¢çš„å­é“¾æ¥
        :param main_url: ä¸»é¡µé¢URL
        :return: å­é“¾æ¥åˆ—è¡¨
        """
        pass
    
    @abstractmethod
    def download_from_sublink(self, sub_link_info):
        """
        ä»å­é“¾æ¥ä¸‹è½½å†…å®¹
        :param sub_link_info: å­é“¾æ¥ä¿¡æ¯
        """
        pass
    def generate_page_url(self, base_url, page_num):
        """
        ç”ŸæˆæŒ‡å®šé¡µé¢çš„URLï¼ˆé»˜è®¤å®ç°ï¼‰
        :param base_url: åŸºç¡€URL
        :param page_num: é¡µé¢ç¼–å·ï¼ˆ0è¡¨ç¤ºç¬¬ä¸€é¡µï¼‰
        :return: é¡µé¢URL
        """
        if page_num == 0:
            return f"{base_url}index.shtml"
        else:
            return f"{base_url}index_{page_num}.shtml"
    
    def crawl_all_pages(self, base_url, max_pages=10):
        """
        çˆ¬å–æ‰€æœ‰é¡µé¢ï¼ˆæ”¯æŒç¿»é¡µï¼‰
        :param base_url: åŸºç¡€URL
        :param max_pages: æœ€å¤§é¡µé¢æ•°ï¼Œé˜²æ­¢æ— é™å¾ªç¯ï¼Œ0è¡¨ç¤ºæ— é™åˆ¶
        """
        try:
            # å¤„ç†max_pages=0çš„æƒ…å†µï¼ˆè¡¨ç¤ºæ— é™åˆ¶ï¼‰
            if max_pages == 0:
                max_pages = 999999
                self.log("è®¾ç½®ä¸ºæ— é™åˆ¶é¡µæ•°æ¨¡å¼")
            
            if not self.driver:
                self.start_driver()
            
            # ç¬¬ä¸€æ­¥ï¼šç»Ÿè®¡æ‰€æœ‰é¡µé¢çš„å­é“¾æ¥æ€»æ•°
            self.log("æ­£åœ¨ç»Ÿè®¡æ‰€æœ‰é¡µé¢çš„å­é“¾æ¥æ•°é‡...")
            self.total_sub_links_count = self.count_all_sub_links(base_url, max_pages)
            
            if self.total_sub_links_count == 0:
                self.log("æœªæ‰¾åˆ°ä»»ä½•å­é“¾æ¥", 'warning')
                return
            
            self.log(f"å…±æ‰¾åˆ° {self.total_sub_links_count} ä¸ªå­é“¾æ¥ï¼Œå¼€å§‹çˆ¬å–...")
            
            # ç¬¬äºŒæ­¥ï¼šé€é¡µå¤„ç†å­é“¾æ¥
            page_count = 0
            
            while page_count < max_pages:
                if self.is_stopped:
                    self.log("çˆ¬è™«å·²åœæ­¢", 'warning')
                    break
                
                current_url = self.generate_page_url(base_url, page_count)
                
                self.log(f"\n{'='*60}")
                self.log(f"æ­£åœ¨å¤„ç†ç¬¬ {page_count + 1} é¡µ")
                self.log(f"é¡µé¢URL: {current_url}")
                self.log(f"{'='*60}")
                
                # æ£€æŸ¥é¡µé¢æ˜¯å¦å­˜åœ¨
                if not self.check_page_exists(current_url):
                    self.log(f"ç¬¬ {page_count + 1} é¡µä¸å­˜åœ¨ï¼Œåœæ­¢ç¿»é¡µ", 'warning')
                    break
                
                self.log(f"ç¬¬ {page_count + 1} é¡µå­˜åœ¨ï¼Œå¼€å§‹çˆ¬å–...")
                
                # è·å–å½“å‰é¡µé¢çš„æ‰€æœ‰å­é“¾æ¥
                sub_links = self.get_sub_links(current_url)
                
                if not sub_links:
                    self.log(f"ç¬¬ {page_count + 1} é¡µæœªæ‰¾åˆ°ä»»ä½•å­é“¾æ¥", 'warning')
                    page_count += 1
                    continue
                
                self.log(f"ç¬¬ {page_count + 1} é¡µæ‰¾åˆ° {len(sub_links)} ä¸ªå­é“¾æ¥")
                
                # è®°å½•å·²å¤„ç†çš„é¡µé¢
                self.stats['pages_processed'].append({
                    'page_num': page_count + 1,
                    'url': current_url,
                    'sub_links_count': len(sub_links)
                })
                
                # é€ä¸ªå¤„ç†å­é“¾æ¥
                for i, sub_link in enumerate(sub_links, 1):
                    if self.is_stopped:
                        self.log("çˆ¬è™«å·²åœæ­¢", 'warning')
                        break
                        
                    self.log(f"\nç¬¬ {page_count + 1} é¡µ - å¤„ç†é“¾æ¥ {i}/{len(sub_links)}: {sub_link.get('title', '')}")
                    
                    self.download_from_sublink(sub_link)
                    
                    # æ›´æ–°è¿›åº¦
                    self.update_sub_link_progress(1)
                    
                    time.sleep(2)  # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                
                self.log(f"ç¬¬ {page_count + 1} é¡µå¤„ç†å®Œæˆ")
                page_count += 1
                time.sleep(3)  # é¡µé¢é—´éš”æ—¶é—´
            
            # æ›´æ–°æ€»é¡µæ•°
            self.stats['total_pages'] = page_count
            
        except Exception as e:
            self.log(f"ç¿»é¡µçˆ¬å–è¿‡ç¨‹ä¸­å‡ºé”™: {e}", 'error')
        finally:
            try:
                self.print_summary_report()
            except Exception as report_error:
                self.log(f"è¾“å‡ºæ€»ç»“æŠ¥å‘Šæ—¶å‡ºé”™: {report_error}", 'error')
            finally:
                self.close_driver()
    
    def crawl_all(self, main_url, max_links=None):
        """
        çˆ¬å–å†…å®¹ï¼ˆå…¼å®¹æ–¹æ³•ï¼Œå†…éƒ¨è°ƒç”¨å¤šé¡µé¢é€»è¾‘ï¼‰
        :param main_url: ä¸»é¡µé¢URLï¼ˆä¼šè¢«è½¬æ¢ä¸ºbase_urlï¼‰
        :param max_links: æœ€å¤§å¤„ç†é“¾æ¥æ•°ï¼ŒNoneè¡¨ç¤ºå¤„ç†æ‰€æœ‰é“¾æ¥
        """
        # å°†main_urlè½¬æ¢ä¸ºbase_urlæ ¼å¼
        # ç§»é™¤æ–‡ä»¶åéƒ¨åˆ†ï¼Œä¿ç•™ç›®å½•è·¯å¾„
        if main_url.endswith('/'):
            base_url = main_url
        else:
            # ç§»é™¤æœ€åä¸€ä¸ª/ä¹‹åçš„éƒ¨åˆ†
            base_url = '/'.join(main_url.split('/')[:-1]) + '/'
        
        # è°ƒç”¨ç»Ÿä¸€çš„å¤šé¡µé¢é€»è¾‘
        self.crawl_all_pages(base_url, max_pages=10)  # ä½¿ç”¨é»˜è®¤çš„æœ€å¤§é¡µæ•° 